{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 DATA SET CODIESP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "# from yellowbrick.text import TSNEVisualizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#import dill\n",
    "# dill.load_session(\"assignment2.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "df_train_clinical_P = pd.read_csv(r\"C:\\Users\\Bluetab\\Desktop\\TFM\\codiesp\\final_dataset_v4_to_publish\\train\\trainP.tsv\", header=None, sep=\"\\t\")\n",
    "\n",
    "df_train_P = pd.DataFrame()\n",
    "def load_articles(x,i):\n",
    "    path = Path(r'C:\\Users\\Bluetab\\Desktop\\TFM\\codiesp\\final_dataset_v4_to_publish\\train\\text_files_en') / x[0][i]\n",
    "    path = str(path) +'.txt'\n",
    "    df_1 = pd.read_csv(path, header=None, sep=\"\\n\")\n",
    "    df_def = df_1.values[0] \n",
    "    for j in range(1, len(df_1)):\n",
    "        df_def = df_def + df_1.values[j]\n",
    "    return pd.DataFrame(df_def)\n",
    "\n",
    "count= 0\n",
    "for i in range(0, len(df_train_clinical_P)):\n",
    "    df_2 = load_articles(df_train_clinical_P, i)\n",
    "    df_train_P = df_train_P.append(df_2)\n",
    "    count = count + 1\n",
    "\n",
    "\n",
    "df_train_P = df_train_P.set_axis(np.array(range(0,len(df_train_clinical_P))))\n",
    "df_train_P[1] = df_train_clinical_P[1]\n",
    "df_train_P.columns = ['content', 'labels']\n",
    "df_train_P.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "df_test_clinical_P = pd.read_csv(r\"C:\\Users\\Bluetab\\Desktop\\TFM\\codiesp\\final_dataset_v4_to_publish\\test\\testP.tsv\", header=None, sep=\"\\t\")\n",
    "\n",
    "df_test_P = pd.DataFrame()\n",
    "def load_articles(x,i):\n",
    "    path = Path(r'C:\\Users\\Bluetab\\Desktop\\TFM\\codiesp\\final_dataset_v4_to_publish\\test\\text_files_en') / x[0][i]\n",
    "    path = str(path) +'.txt'\n",
    "    df_1 = pd.read_csv(path, header=None, sep=\"\\n\", dtype='unicode', error_bad_lines=False)\n",
    "    df_def = df_1.values[0] \n",
    "    for j in range(1, len(df_1)):\n",
    "        df_def = df_def + df_1.values[j]\n",
    "    return pd.DataFrame(df_def)\n",
    "\n",
    "count= 0\n",
    "for i in range(0, len(df_test_clinical_P)):\n",
    "    df_2 = load_articles(df_test_clinical_P, i)\n",
    "    df_test_P = df_test_P.append(df_2)\n",
    "    count = count + 1\n",
    "\n",
    "\n",
    "df_test_P = df_test_P.set_axis(np.array(range(0,len(df_test_clinical_P))))\n",
    "df_test_P[1] = df_test_clinical_P[1]\n",
    "df_test_P.columns = ['content', 'labels']\n",
    "df_test_P.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEV P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "df_dev_clinical_P = pd.read_csv(r\"C:\\Users\\Bluetab\\Desktop\\TFM\\codiesp\\final_dataset_v4_to_publish\\dev\\devP.tsv\", header=None, sep=\"\\t\")\n",
    "\n",
    "df_dev_P = pd.DataFrame()\n",
    "def load_articles(x,i):\n",
    "    try:\n",
    "        path = Path(r'C:\\Users\\Bluetab\\Desktop\\TFM\\codiesp\\final_dataset_v4_to_publish\\dev\\text_files_en') / x[0][i]\n",
    "        path = str(path) +'.txt'\n",
    "        df_1 = pd.read_csv(path, header=None, sep=\"\\n\", dtype='unicode', error_bad_lines=False)\n",
    "        df_def = df_1.values[0] \n",
    "        for j in range(1, len(df_1)):\n",
    "            df_def = df_def + df_1.values[j]\n",
    "        return pd.DataFrame(df_def)\n",
    "    except:\n",
    "        print(\" \")\n",
    "\n",
    "count= 0\n",
    "for i in range(0, len(df_dev_clinical_P)):\n",
    "    df_2 = load_articles(df_dev_clinical_P, i)\n",
    "    df_dev_P = df_dev_P.append(df_2)\n",
    "    count = count + 1\n",
    "\n",
    "\n",
    "df_dev_P = df_dev_P.set_axis(np.array(range(0,790)))\n",
    "df_dev_P[1] = df_dev_clinical_P[1]\n",
    "df_dev_P.columns = ['content', 'labels']\n",
    "df_dev_P.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MATRIX TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels_train = []\n",
    "new_labels_test = []\n",
    "new_labels_dev = []\n",
    "lista = ['content']\n",
    "for i in range(len(df_train_P['labels'])):\n",
    "    new_labels_train.append([df_train_P['labels'][i]])\n",
    "    lista = lista + new_labels_train[i]\n",
    "for i in range(len(df_test_P['labels'])):\n",
    "    new_labels_test.append([df_test_P['labels'][i]])\n",
    "    lista = lista + new_labels_test[i]\n",
    "for i in range(len(df_dev_P['labels'])):\n",
    "    new_labels_dev.append([df_dev_P['labels'][i]])\n",
    "    lista = lista + new_labels_dev[i]\n",
    "\n",
    "    \n",
    "####### TRAIN #######\n",
    "myvec = np.zeros((len(df_train_P.drop_duplicates('content', ignore_index=True)),len(lista)))\n",
    "auxdf = pd.DataFrame(data = myvec, index=range(len(df_train_P.drop_duplicates('content', ignore_index=True))),columns=range(len(lista)))\n",
    "auxdf.columns = lista\n",
    "auxdf = auxdf.loc[:,~auxdf.columns.duplicated()]\n",
    "print(df_train_P)\n",
    "df_train_P1 = df_train_P\n",
    "df_train_P = df_train_P.drop_duplicates('content', ignore_index=True)\n",
    "\n",
    "auxdf['content'] = df_train_P['content']\n",
    "for j in range(len(auxdf['content'])):\n",
    "    for i in range(len(df_train_P1['content'])):\n",
    "        label = df_train_P1['labels'][i]\n",
    "        if auxdf['content'][j]==df_train_P1['content'][i]:\n",
    "            label = df_train_P1['labels'][i]\n",
    "            auxdf[label][j] = 1 \n",
    "            \n",
    "train_matrix = auxdf            \n",
    "\n",
    "\n",
    "#### TEST #####\n",
    "\n",
    "myvec1 = np.zeros((len(df_test_P.drop_duplicates('content', ignore_index=True)),len(lista)))\n",
    "auxdf1 = pd.DataFrame(data = myvec1, index=range(len(df_test_P.drop_duplicates('content', ignore_index=True))),columns=range(len(lista)))\n",
    "auxdf1.columns = lista\n",
    "auxdf1 = auxdf1.loc[:,~auxdf1.columns.duplicated()]\n",
    "df_test_P1 = df_test_P\n",
    "df_test_P = df_test_P.drop_duplicates('content', ignore_index=True)\n",
    "\n",
    "auxdf1['content'] = df_test_P['content']\n",
    "\n",
    "for j in range(len(auxdf1['content'])):\n",
    "    for i in range(len(df_test_P1['content'])):\n",
    "        label1 = df_test_P1['labels'][i]\n",
    "        if auxdf1['content'][j]==df_test_P1['content'][i]:\n",
    "            label1 = df_test_P1['labels'][i]\n",
    "            auxdf1[label1][j] = 1 \n",
    "            \n",
    "test_matrix = auxdf1            \n",
    "\n",
    "\n",
    "###### DEV ######\n",
    "myvec3 = np.zeros((len(df_dev_P.drop_duplicates('content', ignore_index=True)),len(lista)))\n",
    "auxdf3 = pd.DataFrame(data = myvec3, index=range(len(df_dev_P.drop_duplicates('content', ignore_index=True))),columns=range(len(lista)))\n",
    "auxdf3.columns = lista\n",
    "auxdf3 = auxdf3.loc[:,~auxdf3.columns.duplicated()]\n",
    "df_dev_P1 = df_dev_P\n",
    "df_dev_P = df_dev_P.drop_duplicates('content', ignore_index=True)\n",
    "\n",
    "auxdf3['content'] = df_dev_P['content']\n",
    "print(df_dev_P1)\n",
    "for j in range(len(auxdf3['content'])):\n",
    "    for i in range(len(df_dev_P1['content'])):\n",
    "        label3 = df_dev_P1['labels'][i]\n",
    "        if auxdf3['content'][j]==df_dev_P1['content'][i]:\n",
    "            label3 = df_dev_P1['labels'][i]\n",
    "            auxdf3[label3][j] = 1 \n",
    "            \n",
    "dev_matrix = auxdf3            \n",
    "\n",
    "##### TRAIN MATRIX #####\n",
    "df_train_clinical = pd.DataFrame()\n",
    "train_matrix = pd.concat([train_matrix, dev_matrix], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix1 = train_matrix\n",
    "test_matrix1 = test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "for i in range(train_matrix1.shape[1]):\n",
    "    if i>0:\n",
    "        total.append(sum(train_matrix1.iloc[:,i]))\n",
    "\n",
    "\n",
    "total_matrix = pd.concat([train_matrix1, test_matrix1], ignore_index=True)\n",
    "lista = total_matrix.columns\n",
    "lista = lista.delete(0)\n",
    "\n",
    "mat = pd.DataFrame(total, index = lista)\n",
    "mat = mat.sort_values(0, ascending = False)\n",
    "print(mat.iloc[:45])\n",
    "#maxi = len(lista) # coger todas\n",
    "maxi = 45 # coger 10 o m√°s apariciones. Usar print de total para ver hasta donde coger\n",
    "\n",
    "coger = mat.iloc[:maxi]\n",
    "coger = coger.index\n",
    "coger = coger.insert(0,'content')\n",
    "\n",
    "train_matrix = train_matrix[coger]\n",
    "test_matrix = test_matrix[coger]\n",
    "print(train_matrix)\n",
    "# print(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train = 434\n",
    "docs_dev = 220 #654-434\n",
    "docs_test = 222\n",
    "\n",
    "train_words = []\n",
    "dev_words = []\n",
    "test_words = []\n",
    "\n",
    "for i in range(len(train_matrix['content'])):\n",
    "    if i<434:\n",
    "        train_words.append(len(train_matrix['content'][i].strip().split(\" \")))\n",
    "    else:\n",
    "        dev_words.append(len(train_matrix['content'][i].strip().split(\" \")))\n",
    "\n",
    "for i in range(len(test_matrix['content'])):\n",
    "    test_words.append(len(test_matrix['content'][i].strip().split(\" \")))\n",
    "\n",
    "print('Number of words in train: ',sum(train_words))\n",
    "print('Number of words in dev: ', sum(dev_words))\n",
    "print('Number of words in test: ', sum(test_words))\n",
    "\n",
    "print('\\nAverage of words in train: ', round(sum(train_words)/docs_train,2))\n",
    "print('Average of words in dev: ', round(sum(dev_words)/docs_dev,2))\n",
    "print('Average of words in test: ', round(sum(test_words)/docs_test,2))\n",
    "\n",
    "print('\\nStandard deviation of words in train: ', round(np.std(train_words),2))\n",
    "print('Standard deviation of words in dev: ', round(np.std(dev_words),2))\n",
    "print('Standard deviation of words in test: ', round(np.std(test_words),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_train_P1[\"labels\"].value_counts()\n",
    "b = 0\n",
    "print(a)\n",
    "for i in range(len(a)):\n",
    "    if a[i] == 1 or a[i] == 2:\n",
    "        b += 1\n",
    "        \n",
    "print(b*100/len(a))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test_P\n",
    "df_train = df_train_P\n",
    "s = df_test_P[\"labels\"]\n",
    "print(s.value_counts())\n",
    "s.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "wordnet_lemmatizer = WordNetLemmatizer() # resumir?\n",
    "stemmer = PorterStemmer() # sacar la raiz de las palabras\n",
    "stopwords = set(stopwords.words('english')) # creo que es borrar las palabras de I, you, he...\n",
    "\n",
    "def tokenize_lemma_stopwords(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # split string into words (tokens)\n",
    "    tokens = nltk.tokenize.word_tokenize(text.lower())\n",
    "    # keep strings with only alphabets (remove ages)\n",
    "#     tokens = [t for t in tokens if t.isalpha()]\n",
    "    # put words into base form\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] \n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    # remove short words, they're probably not useful\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n",
    "    cleanedText = \" \".join(tokens)\n",
    "    return cleanedText\n",
    "\n",
    "def dataCleaning(df):\n",
    "    data = df.copy()\n",
    "    data[\"content\"] =data[\"content\"].apply(tokenize_lemma_stopwords)\n",
    "    return data\n",
    "cleanedTrainData = dataCleaning(train_matrix)\n",
    "cleanedTestData = dataCleaning(test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorised_train_documents = vectorizer.fit_transform(cleanedTrainData[\"content\"]) #learn vocabulary and idf, return document-term matrix\n",
    "print(vectorised_train_documents)\n",
    "vectorised_test_documents = vectorizer.transform(cleanedTestData[\"content\"]) # only transform the documents to docuemnt-term matrix\n",
    "print(vectorised_test_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YellowBrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.text import FreqDistVisualizer\n",
    "from yellowbrick.text import UMAPVisualizer\n",
    "import umap.umap_ as umap\n",
    "\n",
    "features = vectorizer.get_feature_names() #mapping from feature integer indeces to feature name\n",
    "visualizer = FreqDistVisualizer(features=features, orient='v', n=30)\n",
    "visualizer.fit(vectorised_train_documents)\n",
    "visualizer.ax.tick_params(axis = 'x',labelrotation=80, labelsize = 16)\n",
    "visualizer.ax.tick_params(axis = 'y', labelsize = 14)\n",
    "plt.savefig('plot_word_distribution_P_english.png', dpi=300, bbox_inches='tight')\n",
    "visualizer.show()\n",
    "\n",
    "umap = UMAPVisualizer(metric=\"cosine\") # Uniform Manifold Approximation and Projecytion (clusters can be seen and no correlation)\n",
    "umap.fit(vectorised_train_documents)\n",
    "umap.ax.tick_params(axis = 'x', labelsize = 16)\n",
    "umap.ax.tick_params(axis = 'y', labelsize = 16)\n",
    "plt.savefig('plot_umap_P_english.png', dpi=300, bbox_inches='tight')\n",
    "umap.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, hamming_loss, average_precision_score\n",
    "ModelsPerformance= {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, hamming_loss\n",
    "\n",
    "\n",
    "knnClf = KNeighborsClassifier()\n",
    "\n",
    "knnClf.fit(vectorised_train_documents, train_matrix.iloc[:,1:])\n",
    "knnPredictions = knnClf.predict(vectorised_test_documents)\n",
    "# print(knnPredictions.shape)\n",
    "# print(test_matrix.iloc[:,1:].shape)\n",
    "macro_f1 = f1_score(test_matrix.iloc[:,1:], knnPredictions, average='macro')\n",
    "micro_f1 = f1_score(test_matrix.iloc[:,1:], knnPredictions, average='micro')\n",
    "ap_macro = average_precision_score(test_matrix.iloc[:,1:].to_numpy().reshape((-1)), knnPredictions.reshape((-1)), average='macro')\n",
    "hamLoss = hamming_loss(test_matrix.iloc[:,1:], knnPredictions)\n",
    "ModelsPerformance['KNN'] = [\"micro_f1\", micro_f1], [\"macro_f1\", macro_f1], [\"hamloss\", hamLoss], [\"ap\", ap_macro]\n",
    "print(ModelsPerformance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtClassifier = DecisionTreeClassifier()\n",
    "dtClassifier.fit(vectorised_train_documents, train_matrix.iloc[:,1:])\n",
    "dtPreds = dtClassifier.predict(vectorised_test_documents)\n",
    "\n",
    "macro_f1 = f1_score(test_matrix.iloc[:,1:], dtPreds, average='macro')\n",
    "micro_f1 = f1_score(test_matrix.iloc[:,1:], dtPreds, average='micro')\n",
    "ap_macro = average_precision_score(test_matrix.iloc[:,1:].to_numpy().reshape((-1)), dtPreds.reshape((-1)), average='macro')\n",
    "hamLoss = hamming_loss(test_matrix.iloc[:,1:], dtPreds)\n",
    "ModelsPerformance['Decision Tree'] = [\"micro_f1\", micro_f1], [\"macro_f1\", macro_f1], [\"hamloss\", hamLoss], [\"ap\", ap_macro]\n",
    "print(ModelsPerformance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfClassifier = RandomForestClassifier(n_jobs=-1)\n",
    "rfClassifier.fit(vectorised_train_documents, train_matrix.iloc[:,1:])\n",
    "rfPreds = rfClassifier.predict(vectorised_test_documents)\n",
    "\n",
    "macro_f1 = f1_score(test_matrix.iloc[:,1:], rfPreds, average='macro')\n",
    "micro_f1 = f1_score(test_matrix.iloc[:,1:], rfPreds, average='micro')\n",
    "ap_macro = average_precision_score(test_matrix.iloc[:,1:].to_numpy().reshape((-1)), rfPreds.reshape((-1)), average='macro')\n",
    "hamLoss = hamming_loss(test_matrix.iloc[:,1:], rfPreds)\n",
    "ModelsPerformance['RF'] = [\"micro_f1\", micro_f1], [\"macro_f1\", macro_f1], [\"hamloss\", hamLoss], [\"ap\", ap_macro]\n",
    "print(ModelsPerformance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_dict(d, indent=0):\n",
    "   for key, value in d.items():\n",
    "      print('\\t' * indent + str(key))\n",
    "      if isinstance(value, dict):\n",
    "         pretty(value, indent+1)\n",
    "      else:\n",
    "         print('\\t' * (indent+1) + str(value))\n",
    "            \n",
    "pretty_dict(ModelsPerformance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baggings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagClassifier = OneVsRestClassifier(BaggingClassifier(n_jobs=-1))\n",
    "bagClassifier.fit(vectorised_train_documents, train_matrix.iloc[:,1:])\n",
    "bagPreds = bagClassifier.predict(vectorised_test_documents)\n",
    "\n",
    "macro_f1 = f1_score(test_matrix.iloc[:,1:], bagPreds, average='macro')\n",
    "micro_f1 = f1_score(test_matrix.iloc[:,1:], bagPreds, average='micro')\n",
    "ap_macro = average_precision_score(test_matrix.iloc[:,1:].to_numpy().reshape((-1)), bagPreds.reshape((-1)), average='macro')\n",
    "hamLoss = hamming_loss(test_matrix.iloc[:,1:], bagPreds)\n",
    "ModelsPerformance['Bagging'] = [\"micro_f1\", micro_f1], [\"macro_f1\", macro_f1], [\"hamloss\", hamLoss], [\"ap\", ap_macro]\n",
    "print(ModelsPerformance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagClassifier.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "boostClassifier = OneVsRestClassifier(GradientBoostingClassifier())\n",
    "boostClassifier.fit(vectorised_train_documents, train_matrix.iloc[:,1:])\n",
    "boostPreds = boostClassifier.predict(vectorised_test_documents)\n",
    "\n",
    "macro_f1 = f1_score(test_matrix.iloc[:,1:], boostPreds, average='macro')\n",
    "micro_f1 = f1_score(test_matrix.iloc[:,1:], boostPreds, average='micro')\n",
    "ap_macro = average_precision_score(test_matrix.iloc[:,1:].to_numpy().reshape((-1)), boostPreds.reshape((-1)), average='macro')\n",
    "hamLoss = hamming_loss(test_matrix.iloc[:,1:], boostPreds)\n",
    "ModelsPerformance['Gradient Boosting'] = [\"micro_f1\", micro_f1], [\"macro_f1\", macro_f1], [\"hamloss\", hamLoss], [\"ap\", ap_macro]\n",
    "print(ModelsPerformance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svmClassifier = OneVsRestClassifier(LinearSVC(), n_jobs=-1)\n",
    "svmClassifier.fit(vectorised_train_documents, train_matrix.iloc[:,1:])\n",
    "svmPreds = svmClassifier.predict(vectorised_test_documents)\n",
    "\n",
    "macro_f1 = f1_score(test_matrix.iloc[:,1:], svmPreds, average='macro')\n",
    "micro_f1 = f1_score(test_matrix.iloc[:,1:], svmPreds, average='micro')\n",
    "ap_macro = average_precision_score(test_matrix.iloc[:,1:].to_numpy().reshape((-1)), svmPreds.reshape((-1)), average='macro')\n",
    "hamLoss = hamming_loss(test_matrix.iloc[:,1:], svmPreds)\n",
    "ModelsPerformance['Linear SVC'] = [\"micro_f1\", micro_f1], [\"macro_f1\", macro_f1], [\"hamloss\", hamLoss], [\"ap\", ap_macro]\n",
    "print(ModelsPerformance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_dict(ModelsPerformance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import optuna\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "min_n_estimators = 2\n",
    "max_n_estimators = 50\n",
    "\n",
    "max_depth = 25\n",
    "\n",
    "max_splits=2\n",
    "min_splits=40\n",
    "\n",
    "budget=12\n",
    "def objective_gb(trial):\n",
    "    \n",
    "    n_estimators_suggest = trial.suggest_int(\"n_estimators\", min_n_estimators, max_n_estimators)\n",
    "    \n",
    "    max_depth_suggest = trial.suggest_int(\"max_depth\",2,max_depth)\n",
    "    \n",
    "#     min_samples_split_suggest = trial.suggest_int(\"min_samples_split\", max_splits,min_splits)\n",
    "    \n",
    "    loss_suggest=trial.suggest_categorical(\"loss\", [ 'deviance', 'exponential'])\n",
    "    \n",
    "    criterion_suggest = trial.suggest_categorical(\"criterion\",['friedman_mse', 'mse', 'mae'])\n",
    "    \n",
    "#     imputer_gb_opt = SimpleImputer(strategy = strategy_suggest)\n",
    "    \n",
    "    gb_opt = OneVsRestClassifier(GradientBoostingClassifier(max_depth=max_depth_suggest,\n",
    "#                                         min_samples_split=min_samples_split_suggest,\n",
    "                                        n_estimators = n_estimators_suggest,\n",
    "                                        criterion=criterion_suggest,\n",
    "                                        loss = loss_suggest))\n",
    "    \n",
    "    gb_opt.fit(vectorised_train_documents, train_matrix.iloc[:,1:])\n",
    "    boostPreds = gb_opt.predict(vectorised_test_documents)\n",
    "    micro_f1 = f1_score(test_matrix.iloc[:,1:], boostPreds, average='micro')\n",
    "    return micro_f1\n",
    "\n",
    "start=time.time()\n",
    "sampler=optuna.samplers.TPESampler(seed=0)\n",
    "study_gb = optuna.create_study(direction=\"maximize\",sampler=sampler)\n",
    "np.random.seed(10)\n",
    "study_gb.optimize(objective_gb, n_trials=budget)\n",
    "end=time.time()\n",
    "time_gb_opt=end-start\n",
    "\n",
    "ModelsPerformance['GB_optuna'] = ['f1_micro', study_gb.best_value], ['params', study_gb.best_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "params = study_bgg.best_params\n",
    "\n",
    "boostClassifier = OneVsRestClassifier(GradientBoostingClassifier(**params))\n",
    "boostClassifier.fit(vectorised_train_documents, train_matrix.iloc[:,1:])\n",
    "boostPreds = boostClassifier.predict(vectorised_test_documents)\n",
    "\n",
    "macro_f1 = f1_score(test_matrix.iloc[:,1:], boostPreds, average='macro')\n",
    "micro_f1 = f1_score(test_matrix.iloc[:,1:], boostPreds, average='micro')\n",
    "ap_macro = average_precision_score(test_matrix.iloc[:,1:].to_numpy().reshape((-1)), boostPreds.reshape((-1)), average='macro')\n",
    "ap_micro = average_precision_score(test_matrix.iloc[:,1:].to_numpy().reshape((-1)), boostPreds.reshape((-1)), average='micro')\n",
    "hamLoss = hamming_loss(test_matrix.iloc[:,1:], boostPreds)\n",
    "ModelsPerformance['Gradient Boosting'] = [\"micro_f1\", micro_f1], [\"macro_f1\", macro_f1], [\"hamloss\", hamLoss], [\"ap_micro\", ap_micro], [\"ap_macro\", ap_macro]\n",
    "print(ModelsPerformance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_dict(ModelsPerformance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import optuna\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "min_n_estimators = 2\n",
    "max_n_estimators = 50\n",
    "\n",
    "max_feature = 25\n",
    "\n",
    "max_splits=2\n",
    "min_splits=40\n",
    "\n",
    "budget=12\n",
    "def objective_bgg(trial):\n",
    "    \n",
    "    n_estimators_suggest = trial.suggest_int(\"n_estimators\", min_n_estimators, max_n_estimators)\n",
    "    \n",
    "#     max_feature_suggest = trial.suggest_int(\"max_feature\",2,max_feature)\n",
    "    \n",
    "#     min_samples_split_suggest = trial.suggest_int(\"min_samples_split\", max_splits,min_splits)\n",
    "    \n",
    "    bootstrap_suggest=trial.suggest_categorical(\"bootstrap\", [True,False])\n",
    "    \n",
    "#     criterion_suggest = trial.suggest_categorical(\"criterion\",['friedman_mse', 'mse', 'mae'])\n",
    "    \n",
    "#     imputer_gb_opt = SimpleImputer(strategy = strategy_suggest)\n",
    "    \n",
    "    bging_opt = OneVsRestClassifier(BaggingClassifier(\n",
    "#         max_features=max_feature_suggest,\n",
    "#                                         min_samples_split=min_samples_split_suggest,\n",
    "                                        n_estimators = n_estimators_suggest,\n",
    "                                        bootstrap=bootstrap_suggest\n",
    "                                        ))\n",
    "    \n",
    "    bging_opt.fit(vectorised_train_documents, train_matrix.iloc[:,1:])\n",
    "    bagPreds = bging_opt.predict(vectorised_test_documents)\n",
    "    micro_f1 = f1_score(test_matrix.iloc[:,1:], bagPreds, average='micro')\n",
    "    return micro_f1\n",
    "\n",
    "start=time.time()\n",
    "sampler=optuna.samplers.TPESampler(seed=0)\n",
    "study_bgg = optuna.create_study(direction=\"maximize\",sampler=sampler)\n",
    "np.random.seed(0)\n",
    "study_bgg.optimize(objective_bgg, n_trials=budget)\n",
    "end=time.time()\n",
    "time_gb_opt=end-start\n",
    "\n",
    "ModelsPerformance['Bagg_optuna'] = ['f1_micro', study_bgg.best_value], ['params', study_bgg.best_params]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "params = study_bgg.best_params\n",
    "bagClassifier = OneVsRestClassifier(BaggingClassifier(**params))\n",
    "bagClassifier.fit(vectorised_train_documents, train_matrix.iloc[:,1:])\n",
    "bagPreds = bagClassifier.predict(vectorised_test_documents)\n",
    "\n",
    "macro_f1 = f1_score(test_matrix.iloc[:,1:], bagPreds, average='macro')\n",
    "micro_f1 = f1_score(test_matrix.iloc[:,1:], bagPreds, average='micro')\n",
    "ap_macro = average_precision_score(test_matrix.iloc[:,1:].to_numpy().reshape((-1)), bagPreds.reshape((-1)), average='macro')\n",
    "hamLoss = hamming_loss(test_matrix.iloc[:,1:], bagPreds)\n",
    "ModelsPerformance['Bagging_optuna'] = [\"micro_f1\", micro_f1], [\"macro_f1\", macro_f1], [\"hamloss\", hamLoss], [\"ap\", ap_macro]\n",
    "print(ModelsPerformance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_dict(ModelsPerformance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
