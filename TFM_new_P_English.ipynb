{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"TFM_new_P_English.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"LPK6NhkGUa1S"},"source":["import os\n","import collections\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from datetime import datetime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g2QcRAEoyiAr"},"source":["!pip install -q transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6gX0SwlqyeVH"},"source":["# Importing stock ml libraries\n","import numpy as np\n","import pandas as pd\n","from sklearn import metrics\n","import transformers\n","import torch\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer, BertModel, BertConfig"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLMc17djbndj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629199732806,"user_tz":-120,"elapsed":257,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"eb3449e7-da2e-48f8-a928-e8113b6ca088"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8lDUPGD8Ip4K"},"source":["TRAIN D"]},{"cell_type":"code","metadata":{"id":"4Z8jjfw4Io9X"},"source":["import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","df_train_clinical_P = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/train/trainP.tsv\", header=None, sep=\"\\t\")\n","\n","df_train_P = pd.DataFrame()\n","def load_articles(x,i):\n","    path = Path(r'/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/train/text_files_en') / x[0][i]\n","    path = str(path) +'.txt'\n","    df_1 = pd.read_csv(path, header=None, sep=\"\\n\")\n","    df_def = df_1.values[0] \n","    for j in range(1, len(df_1)):\n","        df_def = df_def + df_1.values[j]\n","    return pd.DataFrame(df_def)\n","\n","count= 0\n","for i in range(0, len(df_train_clinical_P)):\n","    df_2 = load_articles(df_train_clinical_P, i)\n","    df_train_P = df_train_P.append(df_2)\n","    count = count + 1\n","\n","\n","df_train_P = df_train_P.set_axis(np.array(range(0,len(df_train_clinical_P))))\n","df_train_P[1] = df_train_clinical_P[1]\n","df_train_P.columns = ['content', 'labels']\n","df_train_P.head(10)\n","\n","train = df_train_P"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SD5EFsIh6wqI"},"source":["DEV D"]},{"cell_type":"code","metadata":{"id":"BMu-J1vE6y5K"},"source":["import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","df_dev_clinical_P = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/dev/devP.tsv\", header=None, sep=\"\\t\")\n","\n","df_dev_P = pd.DataFrame()\n","def load_articles(x,i):\n","    path = Path(r'/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/dev/text_files_en') / x[0][i]\n","    path = str(path) +'.txt'\n","    df_1 = pd.read_csv(path, header=None, sep=\"\\n\")\n","    df_def = df_1.values[0] \n","    for j in range(1, len(df_1)):\n","        df_def = df_def + df_1.values[j]\n","    return pd.DataFrame(df_def)\n","\n","count= 0\n","for i in range(0, len(df_dev_clinical_P)):\n","    df_2 = load_articles(df_dev_clinical_P, i)\n","    df_dev_P = df_dev_P.append(df_2)\n","    count = count + 1\n","\n","\n","df_dev_P = df_dev_P.set_axis(np.array(range(0,len(df_dev_clinical_P))))\n","df_dev_P[1] = df_dev_clinical_P[1]\n","df_dev_P.columns = ['content', 'labels']\n","df_dev_P.head(10)\n","\n","dev = df_dev_P"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v6Igbf42JTY4"},"source":["TEST D"]},{"cell_type":"code","metadata":{"id":"V8XsgblOJUWY"},"source":["from pathlib import Path\n","df_test_clinical_P = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/test/testP.tsv\", header=None, sep=\"\\t\")\n","\n","df_test_P = pd.DataFrame()\n","def load_articles(x,i):\n","    path = Path(r'/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/test/text_files_en') / x[0][i]\n","    path = str(path) +'.txt'\n","    df_1 = pd.read_csv(path, header=None, sep=\"\\n\", dtype='unicode', error_bad_lines=False)\n","    df_def = df_1.values[0] \n","    for j in range(1, len(df_1)):\n","        df_def = df_def + df_1.values[j]\n","    return pd.DataFrame(df_def)\n","\n","count= 0\n","for i in range(0, len(df_test_clinical_P)):\n","    df_2 = load_articles(df_test_clinical_P, i)\n","    df_test_P = df_test_P.append(df_2)\n","    count = count + 1\n","\n","\n","df_test_P = df_test_P.set_axis(np.array(range(0,len(df_test_clinical_P))))\n","df_test_P[1] = df_test_clinical_P[1]\n","LABEL_COLUMNS = ['content', 'labels']\n","df_test_P.columns = LABEL_COLUMNS\n","df_test_P.head(10)\n","\n","test = df_test_P"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tIfFAifwTWSw"},"source":["**MATRIX TRANSFORMATION**"]},{"cell_type":"code","metadata":{"id":"zCW8KdFrTVqJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629199761932,"user_tz":-120,"elapsed":19544,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"72305923-4178-4c9e-b9cc-d0f3ea7fcd10"},"source":["new_labels_train = []\n","new_labels_test = []\n","new_labels_dev = []\n","lista = ['content']\n","for i in range(len(df_train_P['labels'])):\n","    new_labels_train.append([df_train_P['labels'][i]])\n","    lista = lista + new_labels_train[i]\n","for i in range(len(df_test_P['labels'])):\n","    new_labels_test.append([df_test_P['labels'][i]])\n","    lista = lista + new_labels_test[i]\n","for i in range(len(df_dev_P['labels'])):\n","    new_labels_dev.append([df_dev_P['labels'][i]])\n","    lista = lista + new_labels_dev[i]\n","\n","    \n","####### TRAIN #######\n","myvec = np.zeros((len(df_train_P.drop_duplicates('content', ignore_index = True)),len(lista)))\n","auxdf = pd.DataFrame(data = myvec, index=range(len(df_train_P.drop_duplicates('content', ignore_index = True))),columns=range(len(lista)))\n","auxdf.columns = lista\n","auxdf = auxdf.loc[:,~auxdf.columns.duplicated()]\n","\n","df_train_P1 = df_train_P\n","df_train_P = df_train_P.drop_duplicates('content', ignore_index = True)\n","auxdf['content'] = df_train_P['content']\n","\n","for j in range(len(auxdf['content'])):\n","    for i in range(len(df_train_P1['content'])):\n","        label = df_train_P1['labels'][i]\n","        if auxdf['content'][j]==df_train_P1['content'][i]:\n","            label = df_train_P1['labels'][i]\n","            auxdf[label][j] = 1 \n","            \n","train_matrix = auxdf            \n","\n","\n","#### TEST #####\n","myvec1 = np.zeros((len(df_test_P.drop_duplicates('content', ignore_index = True)),len(lista)))\n","auxdf1 = pd.DataFrame(data = myvec1, index=range(len(df_test_P.drop_duplicates('content', ignore_index = True))),columns=range(len(lista)))\n","auxdf1.columns = lista\n","auxdf1 = auxdf1.loc[:,~auxdf1.columns.duplicated()]\n","\n","df_test_P1 = df_test_P\n","df_test_P = df_test_P.drop_duplicates('content', ignore_index = True)\n","auxdf1['content'] = df_test_P['content']\n","\n","for j in range(len(auxdf1['content'])):\n","    for i in range(len(df_test_P1['content'])):\n","        label1 = df_test_P1['labels'][i]\n","        if auxdf1['content'][j]==df_test_P1['content'][i]:\n","            label1 = df_test_P1['labels'][i]\n","            auxdf1[label1][j] = 1 \n","            \n","test_matrix = auxdf1            \n","\n","###### DEV ######\n","myvec3 = np.zeros((len(df_dev_P.drop_duplicates('content', ignore_index = True)),len(lista)))\n","auxdf3 = pd.DataFrame(data = myvec3, index=range(len(df_dev_P.drop_duplicates('content', ignore_index = True))),columns=range(len(lista)))\n","auxdf3.columns = lista\n","auxdf3 = auxdf3.loc[:,~auxdf3.columns.duplicated()]\n","\n","df_dev_P1 = df_dev_P\n","df_dev_P = df_dev_P.drop_duplicates('content', ignore_index = True)\n","auxdf3['content'] = df_dev_P['content']\n","\n","for j in range(len(auxdf3['content'])):\n","    for i in range(len(df_dev_P1['content'])):\n","        label3 = df_dev_P1['labels'][i]\n","        if auxdf3['content'][j]==df_dev_P1['content'][i]:\n","            label = df_dev_P1['labels'][i]\n","            auxdf3[label3][j] = 1 \n","            \n","dev_matrix = auxdf3            \n","\n","\n","##### TRAIN MATRIX #####\n","df_train_clinical = pd.DataFrame()\n","# train_matrix = pd.concat([train_matrix, dev_matrix], ignore_index=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"},{"output_type":"stream","text":["                                               content  ...  cp151zz\n","0    We describe the case of a 37-year-old man with...  ...      0.0\n","1    We report the case of a 29-year-old woman who ...  ...      0.0\n","2    A 36-year-old male, with no relevant past medi...  ...      0.0\n","3    A 42-year-old woman underwent liver transplant...  ...      0.0\n","4    A 65-year-old male presented with an infravesi...  ...      0.0\n","..                                                 ...  ...      ...\n","430  We describe the case of a 47-year-old female p...  ...      0.0\n","431  A 65-year-old male patient diagnosed with OI f...  ...      0.0\n","432  A 72-year-old male in HD program from December...  ...      0.0\n","433  A 33-year-old woman presented to the emergency...  ...      0.0\n","434  We report the case of a 62-year-old patient wh...  ...      0.0\n","\n","[435 rows x 871 columns]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"},{"output_type":"stream","text":["                                               content  ...  cp151zz\n","0    Patient of 70 years old, retired miner, withou...  ...      0.0\n","1    A seventeen-year-old male with no history of i...  ...      0.0\n","2    A 62-year-old female patient, obese, with diab...  ...      0.0\n","3    A 53-year-old man with no relevant past medica...  ...      0.0\n","4    Male patient, 40 years old, with a history of ...  ...      0.0\n","..                                                 ...  ...      ...\n","219  The case described in our study is that of a 7...  ...      0.0\n","220  22-year-old man weight, 172 cm.), active milit...  ...      0.0\n","221  A 53-year-old male, road veteran cyclist, with...  ...      0.0\n","222  A 34-year-old male presented with a two-week h...  ...      0.0\n","223  A 57-year-old male with a history of arterial ...  ...      0.0\n","\n","[224 rows x 871 columns]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:73: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"},{"output_type":"stream","text":["                                               content  ...  cp151zz\n","0    A 29-year-old woman with a history of duodenal...  ...      0.0\n","1    58-year-old male at the time of transplantatio...  ...      0.0\n","2    A 22-year-old male patient, with no history of...  ...      0.0\n","3    A 35-year-old patient was admitted to our emer...  ...      0.0\n","4    A 90-year-old patient came to his urologist in...  ...      0.0\n","..                                                 ...  ...      ...\n","217  A five-year-old male patient who after sufferi...  ...      0.0\n","218  This is a 16-year-old adolescent, basketball p...  ...      0.0\n","219  We describe the case of a 58-year-old woman.Sh...  ...      0.0\n","220  We report the case of a 64-year-old male with ...  ...      1.0\n","221  A 73-year-old woman with a history of hyperten...  ...      0.0\n","\n","[222 rows x 871 columns]\n","                                               content  ...  cp151zz\n","0    We describe the case of a 37-year-old man with...  ...      0.0\n","1    We report the case of a 29-year-old woman who ...  ...      0.0\n","2    A 36-year-old male, with no relevant past medi...  ...      0.0\n","3    A 42-year-old woman underwent liver transplant...  ...      0.0\n","4    A 65-year-old male presented with an infravesi...  ...      0.0\n","..                                                 ...  ...      ...\n","430  We describe the case of a 47-year-old female p...  ...      0.0\n","431  A 65-year-old male patient diagnosed with OI f...  ...      0.0\n","432  A 72-year-old male in HD program from December...  ...      0.0\n","433  A 33-year-old woman presented to the emergency...  ...      0.0\n","434  We report the case of a 62-year-old patient wh...  ...      0.0\n","\n","[435 rows x 871 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fEx_ZbiPunA_"},"source":["train_matrix1 = train_matrix\n","test_matrix1 = test_matrix\n","dev_matrix1 = dev_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WiDnOJk4ucV3"},"source":["total = []\n","for i in range(train_matrix1.shape[1]):\n","    if i>0:\n","        total.append(sum(train_matrix1.iloc[:,i]))\n","\n","\n","total_matrix = pd.concat([train_matrix1, test_matrix1, dev_matrix1], ignore_index=True)\n","lista = total_matrix.columns\n","lista = lista.delete(0)\n","\n","mat = pd.DataFrame(total, index = lista)\n","mat = mat.sort_values(0, ascending = False)\n","\n","maxi = len(lista) # coger 10 o m√°s apariciones. Usar print de total para ver hasta donde coger\n","#maxi = 46\n","\n","coger = mat.iloc[:maxi]\n","coger = coger.index\n","coger = coger.insert(0,'content')\n","\n","train_matrix = train_matrix[coger]\n","test_matrix = test_matrix[coger]\n","dev_matrix = dev_matrix[coger]\n","# print(train_matrix)\n","# print(test_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EVPN0RkqXAHP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629199767216,"user_tz":-120,"elapsed":5293,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"414768a8-8343-4219-8c2c-309db48a5e74"},"source":["!pip install texthero\n","!pip install tweet-preprocessor"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: texthero in /usr/local/lib/python3.7/dist-packages (1.1.0)\n","Requirement already satisfied: gensim<4.0,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.6.0)\n","Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.62.0)\n","Requirement already satisfied: unidecode>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.2.0)\n","Requirement already satisfied: spacy<3.0.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (2.2.4)\n","Requirement already satisfied: pandas>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.1.5)\n","Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.2.2)\n","Requirement already satisfied: plotly>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.4.1)\n","Requirement already satisfied: wordcloud>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.5.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from texthero) (0.22.2.post1)\n","Requirement already satisfied: nltk>=3.3 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.6.2)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.4.1)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.15.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (5.1.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (1.3.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (7.1.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (2019.12.20)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (1.0.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.2->texthero) (2018.9)\n","Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.2.0->texthero) (1.3.3)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (57.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (3.0.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.8.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.23.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.4.1)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (7.4.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.1.3)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (4.6.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (3.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (3.7.4.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (1.24.3)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud>=1.5.0->texthero) (7.1.2)\n","Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.7/dist-packages (0.6.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r0SNgblYW9xI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629199768754,"user_tz":-120,"elapsed":1549,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"ac6e48b0-0cf5-4a13-b5cd-8ec708a6f6d8"},"source":["import re \n","import nltk\n","from wordcloud import WordCloud\n","from nltk.stem import WordNetLemmatizer \n","from textblob import TextBlob,Word\n","from nltk.corpus import words\n","nltk.download('words')\n","import texthero as hero\n","import re\n","from texthero import stopwords\n","\n","from nltk.corpus import wordnet\n","\n","import tensorflow as tf\n","\n","from nltk.corpus import stopwords\n","from nltk.tokenize import TweetTokenizer\n","\n","import tensorflow as tf\n","\n","\n","def lemma_per_pos(sent):\n","    '''function to lemmatize according to part of speech tag'''\n","    tweet_tokenizer=TweetTokenizer()\n","    lemmatizer = nltk.stem.WordNetLemmatizer()\n","    lemmatized_list = [lemmatizer.lemmatize(w) for w in  tweet_tokenizer.tokenize(sent)]\n","    return \" \".join(lemmatized_list)\n","\n","def df_preprocessing(df,feature_col):\n","    '''\n","    Preprocessing of dataframe\n","    '''\n","    stop = set(stopwords.words('english'))\n","    df[feature_col]= (df[feature_col].pipe(hero.lowercase).\n","                      pipe(hero.remove_urls).\n","                      pipe(hero.remove_digits).\n","                      pipe(hero.remove_punctuation).\n","                      pipe(hero.remove_html_tags) )\n","    # lemmatization\n","#     df[feature_col]= [lemma_per_pos(sent) for sent in df[feature_col]]\n","    # df[col_name]= hero.remove_stopwords(df[col_name],custom_stopwords)\n","    return df"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"qKmc9oqiYtdY"},"source":["target_col= train_matrix.columns[1:]\n","feature_col= train_matrix.columns[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8KbteS5WJVQ"},"source":["with tf.device('/GPU:0'):\n","    proc_train_df= df_preprocessing(train_matrix,feature_col)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HCmKl_QtZIB_"},"source":["proc_test_df = df_preprocessing(test_matrix,feature_col)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0AurD85DVOD-"},"source":["from transformers import AutoTokenizer,TFDistilBertModel, DistilBertConfig\n","from transformers import TFAutoModel\n","import tensorflow as tf \n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Dropout\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","#import tensorflow_addons as tfa\n","\n","\n","#Creating tokenizer\n","def create_tokenizer(pretrained_weights='distilbert-base-uncased'):\n","  '''Function to create the tokenizer'''\n","\n","  tokenizer = AutoTokenizer.from_pretrained(pretrained_weights)\n","  return tokenizer\n","\n","#Tokenization of the data\n","def data_tokenization(dataset,feature_col,max_len,tokenizer):\n","    '''dataset: Pandas dataframe with feature name is column name \n","    Pretrained_weights: selected model \n","    RETURN: [input_ids, attention_mask]'''\n","\n","    tokens = dataset[feature_col].apply(lambda x: tokenizer(x,return_tensors='tf', \n","                                                            truncation=True,\n","                                                            padding='max_length',\n","                                                            max_length=max_len, \n","                                                            add_special_tokens=True))\n","    input_ids= []\n","    attention_mask=[]\n","    for item in tokens:\n","        input_ids.append(item['input_ids'])\n","        attention_mask.append(item['attention_mask'])\n","    input_ids, attention_mask=np.squeeze(input_ids), np.squeeze(attention_mask)\n","\n","\n","    return [input_ids,attention_mask]\n","\n","def bert_model(pretrained_weights,max_len,learning_rate):\n","  '''BERT model creation with pretrained weights\n","  INPUT:\n","  pretrained_weights: Language model pretrained weights\n","  max_len: input length '''\n","  print('Model selected:', pretrained_weights)\n","  bert=TFAutoModel.from_pretrained(pretrained_weights)\n","  \n","  # This is must if you would like to train the layers of language models too.\n","  for layer in bert.layers:\n","      layer.trainable = True\n","\n","  # parameter declaration\n","#   step = tf.Variable(0, trainable=False)\n","#   schedule = tf.optimizers.schedules.PiecewiseConstantDecay([10000, 15000], [2e-0, 2e-1, 1e-2])\n","#   # lr and wd can be a function or a tensor\n","#   lr = learning_rate * schedule(step)\n","#   wd = lambda:lr * schedule(step)\n","#   optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n","\n","  optimizer= tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n","#   optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","  # declaring inputs, BERT take input_ids and attention_mask as input\n","  input_ids= Input(shape=(max_len,),dtype=tf.int32,name='input_ids')\n","  attention_mask=Input(shape=(max_len,),dtype=tf.int32,name='attention_mask')\n","\n","  bert= bert(input_ids,attention_mask=attention_mask)\n","  x= bert[0][:,0,:]\n","  x=tf.keras.layers.Dropout(0.1)(x)\n","  x= tf.keras.layers.Dense(128)(x)\n","  x=tf.keras.layers.Dense(64)(x)\n","  x=tf.keras.layers.Dense(32)(x)\n","\n","  output=tf.keras.layers.Dense(maxi,activation='sigmoid')(x)\n","\n","  model=Model(inputs=[input_ids,attention_mask],outputs=[output])\n","  # compiling model \n","  model.compile(optimizer=optimizer,\n","                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE,name='binary_crossentropy'),\n","                metrics=['accuracy'])\n","  return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BxVIN9-nWZdF"},"source":["pretrained_weights='bert-base-uncased'\n","max_len=256\n","epochs=20\n","learning_rate=2e-5\n","batch_size=4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r32leLI-WbE3"},"source":["tokenizer= create_tokenizer(pretrained_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xv6g_Tk-WcIX"},"source":["x_train= data_tokenization(proc_train_df,feature_col,max_len,tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1TxmQBYUbVy_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629199772921,"user_tz":-120,"elapsed":9,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"1003b22b-3cfd-4468-a7b6-6aaaad341061"},"source":["y_train= proc_train_df[target_col].values\n","y_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1., 1., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 1., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]])"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"c3GZUWIrbbrA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629199811303,"user_tz":-120,"elapsed":38385,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"d30e8dc2-d7ee-4030-bd4c-4a6c42a9cbe8"},"source":["bert=bert_model(pretrained_weights,max_len,learning_rate)\n","bert.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model selected: bert-base-uncased\n"],"name":"stdout"},{"output_type":"stream","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f1c522cbf30>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f1c522cbf30>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f1c6d47d9e0> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING: AutoGraph could not transform <function wrap at 0x7f1c6d47d9e0> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n","Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_ids (InputLayer)          [(None, 256)]        0                                            \n","__________________________________________________________________________________________________\n","attention_mask (InputLayer)     [(None, 256)]        0                                            \n","__________________________________________________________________________________________________\n","tf_bert_model (TFBertModel)     TFBaseModelOutputWit 109482240   input_ids[0][0]                  \n","                                                                 attention_mask[0][0]             \n","__________________________________________________________________________________________________\n","tf.__operators__.getitem (Slici (None, 768)          0           tf_bert_model[0][0]              \n","__________________________________________________________________________________________________\n","dropout_37 (Dropout)            (None, 768)          0           tf.__operators__.getitem[0][0]   \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 128)          98432       dropout_37[0][0]                 \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 64)           8256        dense[0][0]                      \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 32)           2080        dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 870)          28710       dense_2[0][0]                    \n","==================================================================================================\n","Total params: 109,619,718\n","Trainable params: 109,619,718\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WiXYUhBObi3Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629200378882,"user_tz":-120,"elapsed":567597,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"027dbc96-1de0-452d-900e-439914783c57"},"source":["with tf.device('/GPU:0'):\n","    bert.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,verbose=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:5017: UserWarning:\n","\n","\"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n","109/109 [==============================] - 40s 250ms/step - loss: 0.6508 - accuracy: 0.0000e+00\n","Epoch 2/20\n","109/109 [==============================] - 28s 258ms/step - loss: 0.5239 - accuracy: 0.0000e+00\n","Epoch 3/20\n","109/109 [==============================] - 28s 254ms/step - loss: 0.3421 - accuracy: 0.0000e+00\n","Epoch 4/20\n","109/109 [==============================] - 28s 253ms/step - loss: 0.1837 - accuracy: 0.0000e+00\n","Epoch 5/20\n","109/109 [==============================] - 28s 255ms/step - loss: 0.0878 - accuracy: 0.0000e+00\n","Epoch 6/20\n","109/109 [==============================] - 28s 255ms/step - loss: 0.0466 - accuracy: 0.0000e+00\n","Epoch 7/20\n","109/109 [==============================] - 28s 253ms/step - loss: 0.0342 - accuracy: 0.0000e+00\n","Epoch 8/20\n","109/109 [==============================] - 28s 254ms/step - loss: 0.0303 - accuracy: 0.0713\n","Epoch 9/20\n","109/109 [==============================] - 28s 254ms/step - loss: 0.0284 - accuracy: 0.1218\n","Epoch 10/20\n","109/109 [==============================] - 28s 254ms/step - loss: 0.0272 - accuracy: 0.1356\n","Epoch 11/20\n","109/109 [==============================] - 28s 254ms/step - loss: 0.0262 - accuracy: 0.1287\n","Epoch 12/20\n","109/109 [==============================] - 28s 254ms/step - loss: 0.0252 - accuracy: 0.1609\n","Epoch 13/20\n","109/109 [==============================] - 28s 255ms/step - loss: 0.0243 - accuracy: 0.1931\n","Epoch 14/20\n","109/109 [==============================] - 28s 254ms/step - loss: 0.0235 - accuracy: 0.2184\n","Epoch 15/20\n","109/109 [==============================] - 28s 255ms/step - loss: 0.0225 - accuracy: 0.2184\n","Epoch 16/20\n","109/109 [==============================] - 28s 255ms/step - loss: 0.0219 - accuracy: 0.2575\n","Epoch 17/20\n","109/109 [==============================] - 28s 255ms/step - loss: 0.0212 - accuracy: 0.2759\n","Epoch 18/20\n","109/109 [==============================] - 28s 255ms/step - loss: 0.0205 - accuracy: 0.2874\n","Epoch 19/20\n","109/109 [==============================] - 28s 255ms/step - loss: 0.0198 - accuracy: 0.3241\n","Epoch 20/20\n","109/109 [==============================] - 28s 254ms/step - loss: 0.0192 - accuracy: 0.3333\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rTNa8ZPfcdmo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629200379317,"user_tz":-120,"elapsed":462,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"f29c11db-956c-43e9-a7fc-83f69ac808ec"},"source":["x_test= data_tokenization(test_matrix,feature_col,max_len,tokenizer)\n","x_test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([[  101,  5776,  1997, ...,  2015,  2024,   102],\n","        [  101,  1037,  9171, ...,     0,     0,     0],\n","        [  101,  1037,  2095, ..., 25320,  2007,   102],\n","        ...,\n","        [  101,  1037,  2095, ..., 22260,  9386,   102],\n","        [  101,  1037,  2095, ...,  5970,  1996,   102],\n","        [  101,  1037,  2095, ...,  2659,  2067,   102]], dtype=int32),\n"," array([[1, 1, 1, ..., 1, 1, 1],\n","        [1, 1, 1, ..., 0, 0, 0],\n","        [1, 1, 1, ..., 1, 1, 1],\n","        ...,\n","        [1, 1, 1, ..., 1, 1, 1],\n","        [1, 1, 1, ..., 1, 1, 1],\n","        [1, 1, 1, ..., 1, 1, 1]], dtype=int32)]"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"bdfTMIm2cmlQ","colab":{"base_uri":"https://localhost:8080/","height":643},"executionInfo":{"status":"ok","timestamp":1629200386055,"user_tz":-120,"elapsed":6746,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"5f1cdfae-d9ee-428a-f9d6-69f118ca8608"},"source":["preds= bert.predict(x_test)\n","submiss_df= pd.DataFrame(preds, columns= target_col)\n","submiss_df"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>bw03zzz</th>\n","      <th>bw40zzz</th>\n","      <th>bw20</th>\n","      <th>bw24</th>\n","      <th>4a02x4z</th>\n","      <th>bn20</th>\n","      <th>b030</th>\n","      <th>bw21</th>\n","      <th>5a1d</th>\n","      <th>bv44zzz</th>\n","      <th>0djd8zz</th>\n","      <th>3e0t3cz</th>\n","      <th>bw00zzz</th>\n","      <th>08j1xzz</th>\n","      <th>08j0xzz</th>\n","      <th>0dj68zz</th>\n","      <th>bt43zzz</th>\n","      <th>0bh1</th>\n","      <th>0tjb8zz</th>\n","      <th>10e0xzz</th>\n","      <th>0ttb</th>\n","      <th>0fb0</th>\n","      <th>0djdxzz</th>\n","      <th>4a00x4z</th>\n","      <th>0bj08zz</th>\n","      <th>0vt0</th>\n","      <th>bt40zzz</th>\n","      <th>4a07x0z</th>\n","      <th>b020</th>\n","      <th>bw25</th>\n","      <th>0tbb</th>\n","      <th>3e0436z</th>\n","      <th>bt4jzzz</th>\n","      <th>b246zzz</th>\n","      <th>5a1d00z</th>\n","      <th>0db6</th>\n","      <th>bw2f</th>\n","      <th>5a19</th>\n","      <th>0b11</th>\n","      <th>bw30</th>\n","      <th>...</th>\n","      <th>0jb6</th>\n","      <th>0dbp</th>\n","      <th>bd14yzz</th>\n","      <th>0d9b</th>\n","      <th>0bbk0zx</th>\n","      <th>b522</th>\n","      <th>d9072zz</th>\n","      <th>bw29</th>\n","      <th>09bk</th>\n","      <th>bw38</th>\n","      <th>cb12</th>\n","      <th>0h0t</th>\n","      <th>0hbt</th>\n","      <th>bh30</th>\n","      <th>0mq6</th>\n","      <th>0psn</th>\n","      <th>0rspxzz</th>\n","      <th>0kx1</th>\n","      <th>0kxf</th>\n","      <th>0rg6</th>\n","      <th>0wbc</th>\n","      <th>0bbl3zx</th>\n","      <th>0d9e</th>\n","      <th>cb12yzz</th>\n","      <th>0bj0</th>\n","      <th>bf40zzz</th>\n","      <th>0b113f4</th>\n","      <th>0b21xfz</th>\n","      <th>0dh5</th>\n","      <th>bf14</th>\n","      <th>b413</th>\n","      <th>b412</th>\n","      <th>b549zzz</th>\n","      <th>bf37</th>\n","      <th>0mtn</th>\n","      <th>dw16</th>\n","      <th>0btg</th>\n","      <th>0b9g8zx</th>\n","      <th>0bb88zx</th>\n","      <th>cp151zz</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.076200</td>\n","      <td>0.229761</td>\n","      <td>0.073685</td>\n","      <td>0.144548</td>\n","      <td>0.031615</td>\n","      <td>0.071795</td>\n","      <td>0.063779</td>\n","      <td>0.098059</td>\n","      <td>0.028526</td>\n","      <td>0.129807</td>\n","      <td>0.040147</td>\n","      <td>0.022982</td>\n","      <td>0.119128</td>\n","      <td>0.005942</td>\n","      <td>0.020420</td>\n","      <td>0.033661</td>\n","      <td>0.081664</td>\n","      <td>0.047366</td>\n","      <td>0.166627</td>\n","      <td>0.013017</td>\n","      <td>0.150087</td>\n","      <td>0.085331</td>\n","      <td>0.078722</td>\n","      <td>0.020115</td>\n","      <td>0.006972</td>\n","      <td>0.013246</td>\n","      <td>0.024091</td>\n","      <td>0.008833</td>\n","      <td>0.030483</td>\n","      <td>0.038475</td>\n","      <td>0.090752</td>\n","      <td>0.013724</td>\n","      <td>0.047940</td>\n","      <td>0.013854</td>\n","      <td>0.008583</td>\n","      <td>0.020934</td>\n","      <td>0.082328</td>\n","      <td>0.010288</td>\n","      <td>0.028521</td>\n","      <td>0.031150</td>\n","      <td>...</td>\n","      <td>0.000105</td>\n","      <td>1.742600e-07</td>\n","      <td>0.000145</td>\n","      <td>0.001630</td>\n","      <td>0.000819</td>\n","      <td>0.001661</td>\n","      <td>0.002292</td>\n","      <td>0.006511</td>\n","      <td>0.006323</td>\n","      <td>0.000218</td>\n","      <td>0.001150</td>\n","      <td>0.001324</td>\n","      <td>0.004382</td>\n","      <td>0.017614</td>\n","      <td>0.000158</td>\n","      <td>0.002679</td>\n","      <td>0.001665</td>\n","      <td>0.000012</td>\n","      <td>0.004411</td>\n","      <td>0.000763</td>\n","      <td>0.000520</td>\n","      <td>0.001889</td>\n","      <td>0.000583</td>\n","      <td>0.000423</td>\n","      <td>0.000938</td>\n","      <td>0.000017</td>\n","      <td>0.000662</td>\n","      <td>0.000025</td>\n","      <td>0.014788</td>\n","      <td>0.001100</td>\n","      <td>0.000326</td>\n","      <td>2.756252e-08</td>\n","      <td>0.000065</td>\n","      <td>0.000005</td>\n","      <td>0.002921</td>\n","      <td>0.000127</td>\n","      <td>0.002211</td>\n","      <td>0.011917</td>\n","      <td>0.000027</td>\n","      <td>0.000066</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.090535</td>\n","      <td>0.279031</td>\n","      <td>0.054833</td>\n","      <td>0.092445</td>\n","      <td>0.062142</td>\n","      <td>0.022985</td>\n","      <td>0.012398</td>\n","      <td>0.031075</td>\n","      <td>0.039357</td>\n","      <td>0.015635</td>\n","      <td>0.067319</td>\n","      <td>0.019685</td>\n","      <td>0.046776</td>\n","      <td>0.039728</td>\n","      <td>0.018017</td>\n","      <td>0.076910</td>\n","      <td>0.052605</td>\n","      <td>0.040923</td>\n","      <td>0.027143</td>\n","      <td>0.026782</td>\n","      <td>0.018313</td>\n","      <td>0.024324</td>\n","      <td>0.024403</td>\n","      <td>0.012701</td>\n","      <td>0.017722</td>\n","      <td>0.011872</td>\n","      <td>0.018079</td>\n","      <td>0.005607</td>\n","      <td>0.009470</td>\n","      <td>0.017144</td>\n","      <td>0.006041</td>\n","      <td>0.010255</td>\n","      <td>0.008495</td>\n","      <td>0.010611</td>\n","      <td>0.006067</td>\n","      <td>0.025101</td>\n","      <td>0.015009</td>\n","      <td>0.009043</td>\n","      <td>0.010816</td>\n","      <td>0.006677</td>\n","      <td>...</td>\n","      <td>0.000103</td>\n","      <td>3.539725e-08</td>\n","      <td>0.000069</td>\n","      <td>0.000643</td>\n","      <td>0.000547</td>\n","      <td>0.000837</td>\n","      <td>0.000474</td>\n","      <td>0.003039</td>\n","      <td>0.002688</td>\n","      <td>0.000132</td>\n","      <td>0.000477</td>\n","      <td>0.001435</td>\n","      <td>0.002360</td>\n","      <td>0.001783</td>\n","      <td>0.000122</td>\n","      <td>0.000719</td>\n","      <td>0.001945</td>\n","      <td>0.000002</td>\n","      <td>0.004377</td>\n","      <td>0.000141</td>\n","      <td>0.000207</td>\n","      <td>0.000645</td>\n","      <td>0.000162</td>\n","      <td>0.000367</td>\n","      <td>0.000211</td>\n","      <td>0.000003</td>\n","      <td>0.000985</td>\n","      <td>0.000005</td>\n","      <td>0.001506</td>\n","      <td>0.001040</td>\n","      <td>0.000291</td>\n","      <td>6.027709e-09</td>\n","      <td>0.000059</td>\n","      <td>0.000007</td>\n","      <td>0.000698</td>\n","      <td>0.000011</td>\n","      <td>0.000832</td>\n","      <td>0.004295</td>\n","      <td>0.000007</td>\n","      <td>0.000031</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.091225</td>\n","      <td>0.117028</td>\n","      <td>0.054713</td>\n","      <td>0.044690</td>\n","      <td>0.035432</td>\n","      <td>0.022777</td>\n","      <td>0.029730</td>\n","      <td>0.035451</td>\n","      <td>0.038724</td>\n","      <td>0.107767</td>\n","      <td>0.030106</td>\n","      <td>0.141853</td>\n","      <td>0.072146</td>\n","      <td>0.072756</td>\n","      <td>0.071137</td>\n","      <td>0.012262</td>\n","      <td>0.043447</td>\n","      <td>0.019521</td>\n","      <td>0.019115</td>\n","      <td>0.046987</td>\n","      <td>0.011780</td>\n","      <td>0.015166</td>\n","      <td>0.007212</td>\n","      <td>0.037940</td>\n","      <td>0.013285</td>\n","      <td>0.026742</td>\n","      <td>0.010192</td>\n","      <td>0.012351</td>\n","      <td>0.024019</td>\n","      <td>0.018059</td>\n","      <td>0.015846</td>\n","      <td>0.021692</td>\n","      <td>0.042966</td>\n","      <td>0.015938</td>\n","      <td>0.010737</td>\n","      <td>0.008319</td>\n","      <td>0.012514</td>\n","      <td>0.016775</td>\n","      <td>0.010930</td>\n","      <td>0.041152</td>\n","      <td>...</td>\n","      <td>0.000213</td>\n","      <td>4.508391e-07</td>\n","      <td>0.000150</td>\n","      <td>0.001416</td>\n","      <td>0.000603</td>\n","      <td>0.001708</td>\n","      <td>0.001756</td>\n","      <td>0.004202</td>\n","      <td>0.008063</td>\n","      <td>0.000208</td>\n","      <td>0.000689</td>\n","      <td>0.006495</td>\n","      <td>0.009900</td>\n","      <td>0.006598</td>\n","      <td>0.000092</td>\n","      <td>0.003276</td>\n","      <td>0.007233</td>\n","      <td>0.000004</td>\n","      <td>0.008843</td>\n","      <td>0.000379</td>\n","      <td>0.001219</td>\n","      <td>0.000731</td>\n","      <td>0.001286</td>\n","      <td>0.000987</td>\n","      <td>0.000213</td>\n","      <td>0.000025</td>\n","      <td>0.001975</td>\n","      <td>0.000037</td>\n","      <td>0.002930</td>\n","      <td>0.000566</td>\n","      <td>0.000269</td>\n","      <td>3.965030e-08</td>\n","      <td>0.000168</td>\n","      <td>0.000037</td>\n","      <td>0.002601</td>\n","      <td>0.000040</td>\n","      <td>0.003475</td>\n","      <td>0.011163</td>\n","      <td>0.000023</td>\n","      <td>0.000062</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.043238</td>\n","      <td>0.479471</td>\n","      <td>0.225596</td>\n","      <td>0.071669</td>\n","      <td>0.072096</td>\n","      <td>0.088833</td>\n","      <td>0.181723</td>\n","      <td>0.152862</td>\n","      <td>0.131507</td>\n","      <td>0.061086</td>\n","      <td>0.057978</td>\n","      <td>0.029348</td>\n","      <td>0.062164</td>\n","      <td>0.024498</td>\n","      <td>0.054343</td>\n","      <td>0.041163</td>\n","      <td>0.061493</td>\n","      <td>0.045649</td>\n","      <td>0.021999</td>\n","      <td>0.017119</td>\n","      <td>0.054967</td>\n","      <td>0.027260</td>\n","      <td>0.022450</td>\n","      <td>0.055671</td>\n","      <td>0.006729</td>\n","      <td>0.018821</td>\n","      <td>0.013932</td>\n","      <td>0.010509</td>\n","      <td>0.018273</td>\n","      <td>0.042539</td>\n","      <td>0.045139</td>\n","      <td>0.018259</td>\n","      <td>0.027741</td>\n","      <td>0.006059</td>\n","      <td>0.013347</td>\n","      <td>0.037429</td>\n","      <td>0.035720</td>\n","      <td>0.009124</td>\n","      <td>0.025949</td>\n","      <td>0.052905</td>\n","      <td>...</td>\n","      <td>0.000210</td>\n","      <td>1.732749e-07</td>\n","      <td>0.000113</td>\n","      <td>0.000891</td>\n","      <td>0.000360</td>\n","      <td>0.002435</td>\n","      <td>0.003338</td>\n","      <td>0.002658</td>\n","      <td>0.002079</td>\n","      <td>0.000127</td>\n","      <td>0.000557</td>\n","      <td>0.004873</td>\n","      <td>0.005316</td>\n","      <td>0.007927</td>\n","      <td>0.000274</td>\n","      <td>0.002350</td>\n","      <td>0.003320</td>\n","      <td>0.000009</td>\n","      <td>0.002957</td>\n","      <td>0.001179</td>\n","      <td>0.000394</td>\n","      <td>0.000447</td>\n","      <td>0.000766</td>\n","      <td>0.001064</td>\n","      <td>0.001212</td>\n","      <td>0.000015</td>\n","      <td>0.001770</td>\n","      <td>0.000021</td>\n","      <td>0.008385</td>\n","      <td>0.001848</td>\n","      <td>0.000294</td>\n","      <td>3.467522e-08</td>\n","      <td>0.000058</td>\n","      <td>0.000009</td>\n","      <td>0.002054</td>\n","      <td>0.000070</td>\n","      <td>0.002725</td>\n","      <td>0.009685</td>\n","      <td>0.000015</td>\n","      <td>0.000035</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.081013</td>\n","      <td>0.168856</td>\n","      <td>0.063592</td>\n","      <td>0.112886</td>\n","      <td>0.066633</td>\n","      <td>0.071933</td>\n","      <td>0.066752</td>\n","      <td>0.056352</td>\n","      <td>0.034093</td>\n","      <td>0.046116</td>\n","      <td>0.045414</td>\n","      <td>0.012894</td>\n","      <td>0.059734</td>\n","      <td>0.006967</td>\n","      <td>0.013887</td>\n","      <td>0.041575</td>\n","      <td>0.049711</td>\n","      <td>0.028925</td>\n","      <td>0.061823</td>\n","      <td>0.015076</td>\n","      <td>0.086003</td>\n","      <td>0.035710</td>\n","      <td>0.048915</td>\n","      <td>0.010296</td>\n","      <td>0.008518</td>\n","      <td>0.014074</td>\n","      <td>0.022258</td>\n","      <td>0.008343</td>\n","      <td>0.016088</td>\n","      <td>0.018294</td>\n","      <td>0.033189</td>\n","      <td>0.007192</td>\n","      <td>0.011397</td>\n","      <td>0.006588</td>\n","      <td>0.007198</td>\n","      <td>0.014366</td>\n","      <td>0.039709</td>\n","      <td>0.010717</td>\n","      <td>0.014616</td>\n","      <td>0.015553</td>\n","      <td>...</td>\n","      <td>0.000052</td>\n","      <td>4.149693e-08</td>\n","      <td>0.000047</td>\n","      <td>0.000492</td>\n","      <td>0.000460</td>\n","      <td>0.000283</td>\n","      <td>0.001129</td>\n","      <td>0.002053</td>\n","      <td>0.003061</td>\n","      <td>0.000078</td>\n","      <td>0.000442</td>\n","      <td>0.000638</td>\n","      <td>0.002128</td>\n","      <td>0.006637</td>\n","      <td>0.000114</td>\n","      <td>0.000864</td>\n","      <td>0.000624</td>\n","      <td>0.000005</td>\n","      <td>0.002052</td>\n","      <td>0.000282</td>\n","      <td>0.000221</td>\n","      <td>0.000564</td>\n","      <td>0.000282</td>\n","      <td>0.000282</td>\n","      <td>0.000647</td>\n","      <td>0.000006</td>\n","      <td>0.000297</td>\n","      <td>0.000013</td>\n","      <td>0.003055</td>\n","      <td>0.000525</td>\n","      <td>0.000136</td>\n","      <td>6.226098e-09</td>\n","      <td>0.000072</td>\n","      <td>0.000002</td>\n","      <td>0.000661</td>\n","      <td>0.000034</td>\n","      <td>0.000981</td>\n","      <td>0.003562</td>\n","      <td>0.000004</td>\n","      <td>0.000026</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>219</th>\n","      <td>0.114307</td>\n","      <td>0.026204</td>\n","      <td>0.051259</td>\n","      <td>0.024537</td>\n","      <td>0.066708</td>\n","      <td>0.043302</td>\n","      <td>0.047357</td>\n","      <td>0.044678</td>\n","      <td>0.024850</td>\n","      <td>0.024993</td>\n","      <td>0.027525</td>\n","      <td>0.044373</td>\n","      <td>0.021346</td>\n","      <td>0.012865</td>\n","      <td>0.017377</td>\n","      <td>0.014562</td>\n","      <td>0.015718</td>\n","      <td>0.008030</td>\n","      <td>0.015312</td>\n","      <td>0.021775</td>\n","      <td>0.012406</td>\n","      <td>0.036983</td>\n","      <td>0.011786</td>\n","      <td>0.015385</td>\n","      <td>0.008988</td>\n","      <td>0.029745</td>\n","      <td>0.008024</td>\n","      <td>0.011013</td>\n","      <td>0.010815</td>\n","      <td>0.004631</td>\n","      <td>0.017506</td>\n","      <td>0.018560</td>\n","      <td>0.006048</td>\n","      <td>0.011546</td>\n","      <td>0.013983</td>\n","      <td>0.008300</td>\n","      <td>0.015263</td>\n","      <td>0.018213</td>\n","      <td>0.004861</td>\n","      <td>0.015377</td>\n","      <td>...</td>\n","      <td>0.000081</td>\n","      <td>1.215940e-07</td>\n","      <td>0.000057</td>\n","      <td>0.000265</td>\n","      <td>0.000393</td>\n","      <td>0.000199</td>\n","      <td>0.000469</td>\n","      <td>0.000617</td>\n","      <td>0.006267</td>\n","      <td>0.000120</td>\n","      <td>0.000637</td>\n","      <td>0.000837</td>\n","      <td>0.003363</td>\n","      <td>0.004515</td>\n","      <td>0.000062</td>\n","      <td>0.000776</td>\n","      <td>0.001887</td>\n","      <td>0.000002</td>\n","      <td>0.003349</td>\n","      <td>0.000244</td>\n","      <td>0.000713</td>\n","      <td>0.000209</td>\n","      <td>0.000507</td>\n","      <td>0.000261</td>\n","      <td>0.000159</td>\n","      <td>0.000007</td>\n","      <td>0.000480</td>\n","      <td>0.000009</td>\n","      <td>0.001058</td>\n","      <td>0.000296</td>\n","      <td>0.000066</td>\n","      <td>6.821500e-09</td>\n","      <td>0.000132</td>\n","      <td>0.000006</td>\n","      <td>0.000515</td>\n","      <td>0.000017</td>\n","      <td>0.000778</td>\n","      <td>0.002209</td>\n","      <td>0.000006</td>\n","      <td>0.000016</td>\n","    </tr>\n","    <tr>\n","      <th>220</th>\n","      <td>0.080045</td>\n","      <td>0.023091</td>\n","      <td>0.066001</td>\n","      <td>0.017466</td>\n","      <td>0.070475</td>\n","      <td>0.026664</td>\n","      <td>0.022275</td>\n","      <td>0.107405</td>\n","      <td>0.068313</td>\n","      <td>0.018946</td>\n","      <td>0.021348</td>\n","      <td>0.070380</td>\n","      <td>0.010755</td>\n","      <td>0.042899</td>\n","      <td>0.031128</td>\n","      <td>0.025090</td>\n","      <td>0.025892</td>\n","      <td>0.011191</td>\n","      <td>0.008666</td>\n","      <td>0.030840</td>\n","      <td>0.009114</td>\n","      <td>0.023332</td>\n","      <td>0.012540</td>\n","      <td>0.030163</td>\n","      <td>0.017089</td>\n","      <td>0.047802</td>\n","      <td>0.007432</td>\n","      <td>0.012744</td>\n","      <td>0.021283</td>\n","      <td>0.006772</td>\n","      <td>0.010832</td>\n","      <td>0.026275</td>\n","      <td>0.007970</td>\n","      <td>0.027932</td>\n","      <td>0.017151</td>\n","      <td>0.032221</td>\n","      <td>0.009960</td>\n","      <td>0.020862</td>\n","      <td>0.003432</td>\n","      <td>0.025078</td>\n","      <td>...</td>\n","      <td>0.000284</td>\n","      <td>5.230842e-07</td>\n","      <td>0.000173</td>\n","      <td>0.000499</td>\n","      <td>0.001185</td>\n","      <td>0.000813</td>\n","      <td>0.000492</td>\n","      <td>0.001845</td>\n","      <td>0.006781</td>\n","      <td>0.000237</td>\n","      <td>0.001339</td>\n","      <td>0.001914</td>\n","      <td>0.004562</td>\n","      <td>0.003576</td>\n","      <td>0.000117</td>\n","      <td>0.001407</td>\n","      <td>0.004339</td>\n","      <td>0.000003</td>\n","      <td>0.006972</td>\n","      <td>0.000266</td>\n","      <td>0.000894</td>\n","      <td>0.000267</td>\n","      <td>0.001141</td>\n","      <td>0.000544</td>\n","      <td>0.000154</td>\n","      <td>0.000008</td>\n","      <td>0.001902</td>\n","      <td>0.000007</td>\n","      <td>0.001190</td>\n","      <td>0.000997</td>\n","      <td>0.000187</td>\n","      <td>2.207773e-08</td>\n","      <td>0.000213</td>\n","      <td>0.000033</td>\n","      <td>0.000777</td>\n","      <td>0.000029</td>\n","      <td>0.000813</td>\n","      <td>0.005005</td>\n","      <td>0.000008</td>\n","      <td>0.000029</td>\n","    </tr>\n","    <tr>\n","      <th>221</th>\n","      <td>0.057391</td>\n","      <td>0.039518</td>\n","      <td>0.133895</td>\n","      <td>0.019636</td>\n","      <td>0.116334</td>\n","      <td>0.107112</td>\n","      <td>0.085353</td>\n","      <td>0.089271</td>\n","      <td>0.097731</td>\n","      <td>0.017434</td>\n","      <td>0.028796</td>\n","      <td>0.018927</td>\n","      <td>0.021511</td>\n","      <td>0.021418</td>\n","      <td>0.036009</td>\n","      <td>0.030580</td>\n","      <td>0.010217</td>\n","      <td>0.019706</td>\n","      <td>0.008569</td>\n","      <td>0.008822</td>\n","      <td>0.009274</td>\n","      <td>0.008480</td>\n","      <td>0.008066</td>\n","      <td>0.033481</td>\n","      <td>0.016391</td>\n","      <td>0.014198</td>\n","      <td>0.005865</td>\n","      <td>0.024690</td>\n","      <td>0.018513</td>\n","      <td>0.014040</td>\n","      <td>0.021619</td>\n","      <td>0.047007</td>\n","      <td>0.005622</td>\n","      <td>0.008926</td>\n","      <td>0.033294</td>\n","      <td>0.013264</td>\n","      <td>0.006610</td>\n","      <td>0.019484</td>\n","      <td>0.008838</td>\n","      <td>0.034497</td>\n","      <td>...</td>\n","      <td>0.000094</td>\n","      <td>1.167108e-07</td>\n","      <td>0.000083</td>\n","      <td>0.000330</td>\n","      <td>0.000186</td>\n","      <td>0.000279</td>\n","      <td>0.000912</td>\n","      <td>0.000927</td>\n","      <td>0.005959</td>\n","      <td>0.000057</td>\n","      <td>0.000671</td>\n","      <td>0.001965</td>\n","      <td>0.005812</td>\n","      <td>0.002582</td>\n","      <td>0.000141</td>\n","      <td>0.000962</td>\n","      <td>0.002018</td>\n","      <td>0.000001</td>\n","      <td>0.002193</td>\n","      <td>0.000252</td>\n","      <td>0.000787</td>\n","      <td>0.000072</td>\n","      <td>0.000633</td>\n","      <td>0.000953</td>\n","      <td>0.000151</td>\n","      <td>0.000008</td>\n","      <td>0.001145</td>\n","      <td>0.000013</td>\n","      <td>0.001447</td>\n","      <td>0.000530</td>\n","      <td>0.000088</td>\n","      <td>5.642347e-09</td>\n","      <td>0.000241</td>\n","      <td>0.000008</td>\n","      <td>0.000222</td>\n","      <td>0.000020</td>\n","      <td>0.000974</td>\n","      <td>0.005315</td>\n","      <td>0.000005</td>\n","      <td>0.000008</td>\n","    </tr>\n","    <tr>\n","      <th>222</th>\n","      <td>0.078970</td>\n","      <td>0.067945</td>\n","      <td>0.080709</td>\n","      <td>0.046966</td>\n","      <td>0.037742</td>\n","      <td>0.047760</td>\n","      <td>0.033403</td>\n","      <td>0.042262</td>\n","      <td>0.020590</td>\n","      <td>0.081363</td>\n","      <td>0.026931</td>\n","      <td>0.047451</td>\n","      <td>0.042912</td>\n","      <td>0.013811</td>\n","      <td>0.022665</td>\n","      <td>0.017814</td>\n","      <td>0.029719</td>\n","      <td>0.017459</td>\n","      <td>0.038307</td>\n","      <td>0.027390</td>\n","      <td>0.016853</td>\n","      <td>0.035707</td>\n","      <td>0.021712</td>\n","      <td>0.023078</td>\n","      <td>0.007344</td>\n","      <td>0.014498</td>\n","      <td>0.010320</td>\n","      <td>0.008472</td>\n","      <td>0.018381</td>\n","      <td>0.013846</td>\n","      <td>0.031634</td>\n","      <td>0.016121</td>\n","      <td>0.020803</td>\n","      <td>0.014568</td>\n","      <td>0.014442</td>\n","      <td>0.007256</td>\n","      <td>0.022554</td>\n","      <td>0.019740</td>\n","      <td>0.015521</td>\n","      <td>0.024586</td>\n","      <td>...</td>\n","      <td>0.000081</td>\n","      <td>1.227529e-07</td>\n","      <td>0.000058</td>\n","      <td>0.000824</td>\n","      <td>0.000433</td>\n","      <td>0.000586</td>\n","      <td>0.001062</td>\n","      <td>0.002293</td>\n","      <td>0.005559</td>\n","      <td>0.000134</td>\n","      <td>0.000549</td>\n","      <td>0.001541</td>\n","      <td>0.004250</td>\n","      <td>0.005510</td>\n","      <td>0.000078</td>\n","      <td>0.001706</td>\n","      <td>0.001907</td>\n","      <td>0.000002</td>\n","      <td>0.002965</td>\n","      <td>0.000328</td>\n","      <td>0.000680</td>\n","      <td>0.000471</td>\n","      <td>0.000813</td>\n","      <td>0.000463</td>\n","      <td>0.000244</td>\n","      <td>0.000011</td>\n","      <td>0.000672</td>\n","      <td>0.000016</td>\n","      <td>0.002194</td>\n","      <td>0.000485</td>\n","      <td>0.000153</td>\n","      <td>9.062064e-09</td>\n","      <td>0.000065</td>\n","      <td>0.000006</td>\n","      <td>0.001550</td>\n","      <td>0.000035</td>\n","      <td>0.001735</td>\n","      <td>0.005173</td>\n","      <td>0.000014</td>\n","      <td>0.000020</td>\n","    </tr>\n","    <tr>\n","      <th>223</th>\n","      <td>0.052207</td>\n","      <td>0.369585</td>\n","      <td>0.061244</td>\n","      <td>0.031623</td>\n","      <td>0.139528</td>\n","      <td>0.030989</td>\n","      <td>0.043276</td>\n","      <td>0.043305</td>\n","      <td>0.132999</td>\n","      <td>0.013553</td>\n","      <td>0.097591</td>\n","      <td>0.033412</td>\n","      <td>0.086844</td>\n","      <td>0.044533</td>\n","      <td>0.040409</td>\n","      <td>0.035326</td>\n","      <td>0.024558</td>\n","      <td>0.013870</td>\n","      <td>0.007391</td>\n","      <td>0.034526</td>\n","      <td>0.009558</td>\n","      <td>0.030714</td>\n","      <td>0.008829</td>\n","      <td>0.017998</td>\n","      <td>0.007159</td>\n","      <td>0.020046</td>\n","      <td>0.008265</td>\n","      <td>0.008154</td>\n","      <td>0.004799</td>\n","      <td>0.008238</td>\n","      <td>0.005041</td>\n","      <td>0.017038</td>\n","      <td>0.006751</td>\n","      <td>0.002170</td>\n","      <td>0.007976</td>\n","      <td>0.019413</td>\n","      <td>0.008061</td>\n","      <td>0.014018</td>\n","      <td>0.008621</td>\n","      <td>0.010822</td>\n","      <td>...</td>\n","      <td>0.000199</td>\n","      <td>5.511687e-08</td>\n","      <td>0.000040</td>\n","      <td>0.000323</td>\n","      <td>0.000156</td>\n","      <td>0.000512</td>\n","      <td>0.000525</td>\n","      <td>0.000966</td>\n","      <td>0.002222</td>\n","      <td>0.000057</td>\n","      <td>0.000206</td>\n","      <td>0.003322</td>\n","      <td>0.002920</td>\n","      <td>0.004179</td>\n","      <td>0.000088</td>\n","      <td>0.000723</td>\n","      <td>0.002728</td>\n","      <td>0.000001</td>\n","      <td>0.003759</td>\n","      <td>0.000426</td>\n","      <td>0.000183</td>\n","      <td>0.000135</td>\n","      <td>0.000201</td>\n","      <td>0.000381</td>\n","      <td>0.000181</td>\n","      <td>0.000005</td>\n","      <td>0.001143</td>\n","      <td>0.000005</td>\n","      <td>0.000825</td>\n","      <td>0.000372</td>\n","      <td>0.000210</td>\n","      <td>6.154113e-09</td>\n","      <td>0.000104</td>\n","      <td>0.000006</td>\n","      <td>0.001110</td>\n","      <td>0.000010</td>\n","      <td>0.000874</td>\n","      <td>0.002261</td>\n","      <td>0.000004</td>\n","      <td>0.000020</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>224 rows √ó 870 columns</p>\n","</div>"],"text/plain":["      bw03zzz   bw40zzz      bw20  ...   0b9g8zx   0bb88zx   cp151zz\n","0    0.076200  0.229761  0.073685  ...  0.011917  0.000027  0.000066\n","1    0.090535  0.279031  0.054833  ...  0.004295  0.000007  0.000031\n","2    0.091225  0.117028  0.054713  ...  0.011163  0.000023  0.000062\n","3    0.043238  0.479471  0.225596  ...  0.009685  0.000015  0.000035\n","4    0.081013  0.168856  0.063592  ...  0.003562  0.000004  0.000026\n","..        ...       ...       ...  ...       ...       ...       ...\n","219  0.114307  0.026204  0.051259  ...  0.002209  0.000006  0.000016\n","220  0.080045  0.023091  0.066001  ...  0.005005  0.000008  0.000029\n","221  0.057391  0.039518  0.133895  ...  0.005315  0.000005  0.000008\n","222  0.078970  0.067945  0.080709  ...  0.005173  0.000014  0.000020\n","223  0.052207  0.369585  0.061244  ...  0.002261  0.000004  0.000020\n","\n","[224 rows x 870 columns]"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"YrUWcBYwc5vp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629200386056,"user_tz":-120,"elapsed":34,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"a6146a53-2df9-4ef4-be22-a33fadede158"},"source":["outputs = submiss_df>0.5\n","f1_score_micro = metrics.f1_score(test_matrix.iloc[:,1:], outputs, average='micro')\n","f1_score_macro = metrics.f1_score(test_matrix.iloc[:,1:], outputs, average='macro')\n","ap = metrics.average_precision_score(test_matrix.iloc[:,1:], outputs, average='micro')\n","print(f1_score_micro)\n","print(f1_score_macro)\n","print(ap)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.020247469066366704\n","0.0004240102171136653\n","0.013679302779132131\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning:\n","\n","F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","\n"],"name":"stderr"}]}]}