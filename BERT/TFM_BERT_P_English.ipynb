{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"TFM_new_P_English.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"LPK6NhkGUa1S"},"source":["import os\n","import collections\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from datetime import datetime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g2QcRAEoyiAr"},"source":["!pip install -q transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6gX0SwlqyeVH"},"source":["# Importing stock ml libraries\n","import numpy as np\n","import pandas as pd\n","from sklearn import metrics\n","import transformers\n","import torch\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer, BertModel, BertConfig"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLMc17djbndj"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8lDUPGD8Ip4K"},"source":["TRAIN D"]},{"cell_type":"code","metadata":{"id":"4Z8jjfw4Io9X"},"source":["import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","df_train_clinical_P = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/train/trainP.tsv\", header=None, sep=\"\\t\")\n","\n","df_train_P = pd.DataFrame()\n","def load_articles(x,i):\n","    path = Path(r'/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/train/text_files_en') / x[0][i]\n","    path = str(path) +'.txt'\n","    df_1 = pd.read_csv(path, header=None, sep=\"\\n\")\n","    df_def = df_1.values[0] \n","    for j in range(1, len(df_1)):\n","        df_def = df_def + df_1.values[j]\n","    return pd.DataFrame(df_def)\n","\n","count= 0\n","for i in range(0, len(df_train_clinical_P)):\n","    df_2 = load_articles(df_train_clinical_P, i)\n","    df_train_P = df_train_P.append(df_2)\n","    count = count + 1\n","\n","\n","df_train_P = df_train_P.set_axis(np.array(range(0,len(df_train_clinical_P))))\n","df_train_P[1] = df_train_clinical_P[1]\n","df_train_P.columns = ['content', 'labels']\n","df_train_P.head(10)\n","\n","train = df_train_P"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SD5EFsIh6wqI"},"source":["DEV D"]},{"cell_type":"code","metadata":{"id":"BMu-J1vE6y5K"},"source":["import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","df_dev_clinical_P = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/dev/devP.tsv\", header=None, sep=\"\\t\")\n","\n","df_dev_P = pd.DataFrame()\n","def load_articles(x,i):\n","    path = Path(r'/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/dev/text_files_en') / x[0][i]\n","    path = str(path) +'.txt'\n","    df_1 = pd.read_csv(path, header=None, sep=\"\\n\")\n","    df_def = df_1.values[0] \n","    for j in range(1, len(df_1)):\n","        df_def = df_def + df_1.values[j]\n","    return pd.DataFrame(df_def)\n","\n","count= 0\n","for i in range(0, len(df_dev_clinical_P)):\n","    df_2 = load_articles(df_dev_clinical_P, i)\n","    df_dev_P = df_dev_P.append(df_2)\n","    count = count + 1\n","\n","\n","df_dev_P = df_dev_P.set_axis(np.array(range(0,len(df_dev_clinical_P))))\n","df_dev_P[1] = df_dev_clinical_P[1]\n","df_dev_P.columns = ['content', 'labels']\n","df_dev_P.head(10)\n","\n","dev = df_dev_P"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v6Igbf42JTY4"},"source":["TEST D"]},{"cell_type":"code","metadata":{"id":"V8XsgblOJUWY"},"source":["from pathlib import Path\n","df_test_clinical_P = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/test/testP.tsv\", header=None, sep=\"\\t\")\n","\n","df_test_P = pd.DataFrame()\n","def load_articles(x,i):\n","    path = Path(r'/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/test/text_files_en') / x[0][i]\n","    path = str(path) +'.txt'\n","    df_1 = pd.read_csv(path, header=None, sep=\"\\n\", dtype='unicode', error_bad_lines=False)\n","    df_def = df_1.values[0] \n","    for j in range(1, len(df_1)):\n","        df_def = df_def + df_1.values[j]\n","    return pd.DataFrame(df_def)\n","\n","count= 0\n","for i in range(0, len(df_test_clinical_P)):\n","    df_2 = load_articles(df_test_clinical_P, i)\n","    df_test_P = df_test_P.append(df_2)\n","    count = count + 1\n","\n","\n","df_test_P = df_test_P.set_axis(np.array(range(0,len(df_test_clinical_P))))\n","df_test_P[1] = df_test_clinical_P[1]\n","LABEL_COLUMNS = ['content', 'labels']\n","df_test_P.columns = LABEL_COLUMNS\n","df_test_P.head(10)\n","\n","test = df_test_P"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tIfFAifwTWSw"},"source":["**MATRIX TRANSFORMATION**"]},{"cell_type":"code","metadata":{"id":"zCW8KdFrTVqJ"},"source":["new_labels_train = []\n","new_labels_test = []\n","new_labels_dev = []\n","lista = ['content']\n","for i in range(len(df_train_P['labels'])):\n","    new_labels_train.append([df_train_P['labels'][i]])\n","    lista = lista + new_labels_train[i]\n","for i in range(len(df_test_P['labels'])):\n","    new_labels_test.append([df_test_P['labels'][i]])\n","    lista = lista + new_labels_test[i]\n","for i in range(len(df_dev_P['labels'])):\n","    new_labels_dev.append([df_dev_P['labels'][i]])\n","    lista = lista + new_labels_dev[i]\n","\n","    \n","####### TRAIN #######\n","myvec = np.zeros((len(df_train_P.drop_duplicates('content', ignore_index = True)),len(lista)))\n","auxdf = pd.DataFrame(data = myvec, index=range(len(df_train_P.drop_duplicates('content', ignore_index = True))),columns=range(len(lista)))\n","auxdf.columns = lista\n","auxdf = auxdf.loc[:,~auxdf.columns.duplicated()]\n","\n","df_train_P1 = df_train_P\n","df_train_P = df_train_P.drop_duplicates('content', ignore_index = True)\n","auxdf['content'] = df_train_P['content']\n","\n","for j in range(len(auxdf['content'])):\n","    for i in range(len(df_train_P1['content'])):\n","        label = df_train_P1['labels'][i]\n","        if auxdf['content'][j]==df_train_P1['content'][i]:\n","            label = df_train_P1['labels'][i]\n","            auxdf[label][j] = 1 \n","            \n","train_matrix = auxdf            \n","\n","#### TEST #####\n","myvec1 = np.zeros((len(df_test_P.drop_duplicates('content', ignore_index = True)),len(lista)))\n","auxdf1 = pd.DataFrame(data = myvec1, index=range(len(df_test_P.drop_duplicates('content', ignore_index = True))),columns=range(len(lista)))\n","auxdf1.columns = lista\n","auxdf1 = auxdf1.loc[:,~auxdf1.columns.duplicated()]\n","\n","df_test_P1 = df_test_P\n","df_test_P = df_test_P.drop_duplicates('content', ignore_index = True)\n","auxdf1['content'] = df_test_P['content']\n","\n","for j in range(len(auxdf1['content'])):\n","    for i in range(len(df_test_P1['content'])):\n","        label1 = df_test_P1['labels'][i]\n","        if auxdf1['content'][j]==df_test_P1['content'][i]:\n","            label1 = df_test_P1['labels'][i]\n","            auxdf1[label1][j] = 1 \n","            \n","test_matrix = auxdf1            \n","\n","###### DEV ######\n","myvec3 = np.zeros((len(df_dev_P.drop_duplicates('content', ignore_index = True)),len(lista)))\n","auxdf3 = pd.DataFrame(data = myvec3, index=range(len(df_dev_P.drop_duplicates('content', ignore_index = True))),columns=range(len(lista)))\n","auxdf3.columns = lista\n","auxdf3 = auxdf3.loc[:,~auxdf3.columns.duplicated()]\n","\n","df_dev_P1 = df_dev_P\n","df_dev_P = df_dev_P.drop_duplicates('content', ignore_index = True)\n","auxdf3['content'] = df_dev_P['content']\n","\n","for j in range(len(auxdf3['content'])):\n","    for i in range(len(df_dev_P1['content'])):\n","        label3 = df_dev_P1['labels'][i]\n","        if auxdf3['content'][j]==df_dev_P1['content'][i]:\n","            label = df_dev_P1['labels'][i]\n","            auxdf3[label3][j] = 1 \n","            \n","dev_matrix = auxdf3            \n","\n","\n","##### TRAIN MATRIX #####\n","df_train_clinical = pd.DataFrame()\n","# train_matrix = pd.concat([train_matrix, dev_matrix], ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEx_ZbiPunA_"},"source":["train_matrix1 = train_matrix\n","test_matrix1 = test_matrix\n","dev_matrix1 = dev_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WiDnOJk4ucV3"},"source":["total = []\n","for i in range(train_matrix1.shape[1]):\n","    if i>0:\n","        total.append(sum(train_matrix1.iloc[:,i]))\n","\n","\n","total_matrix = pd.concat([train_matrix1, test_matrix1, dev_matrix1], ignore_index=True)\n","lista = total_matrix.columns\n","lista = lista.delete(0)\n","\n","mat = pd.DataFrame(total, index = lista)\n","mat = mat.sort_values(0, ascending = False)\n","\n","maxi = len(lista)\n","#maxi = 46\n","\n","coger = mat.iloc[:maxi]\n","coger = coger.index\n","coger = coger.insert(0,'content')\n","\n","train_matrix = train_matrix[coger]\n","test_matrix = test_matrix[coger]\n","dev_matrix = dev_matrix[coger]\n","# print(train_matrix)\n","# print(test_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EVPN0RkqXAHP"},"source":["!pip install texthero\n","!pip install tweet-preprocessor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r0SNgblYW9xI"},"source":["import re \n","import nltk\n","from wordcloud import WordCloud\n","from nltk.stem import WordNetLemmatizer \n","from textblob import TextBlob,Word\n","from nltk.corpus import words\n","nltk.download('words')\n","import texthero as hero\n","import re\n","from texthero import stopwords\n","\n","from nltk.corpus import wordnet\n","\n","import tensorflow as tf\n","\n","from nltk.corpus import stopwords\n","from nltk.tokenize import TweetTokenizer\n","\n","import tensorflow as tf\n","\n","\n","def lemma_per_pos(sent):\n","    '''function to lemmatize according to part of speech tag'''\n","    tweet_tokenizer=TweetTokenizer()\n","    lemmatizer = nltk.stem.WordNetLemmatizer()\n","    lemmatized_list = [lemmatizer.lemmatize(w) for w in  tweet_tokenizer.tokenize(sent)]\n","    return \" \".join(lemmatized_list)\n","\n","def df_preprocessing(df,feature_col):\n","    '''\n","    Preprocessing of dataframe\n","    '''\n","    stop = set(stopwords.words('english'))\n","    df[feature_col]= (df[feature_col].pipe(hero.lowercase).\n","                      pipe(hero.remove_urls).\n","                      pipe(hero.remove_digits).\n","                      pipe(hero.remove_punctuation).\n","                      pipe(hero.remove_html_tags) )\n","    # lemmatization\n","#     df[feature_col]= [lemma_per_pos(sent) for sent in df[feature_col]]\n","    # df[col_name]= hero.remove_stopwords(df[col_name],custom_stopwords)\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qKmc9oqiYtdY"},"source":["target_col= train_matrix.columns[1:]\n","feature_col= train_matrix.columns[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8KbteS5WJVQ"},"source":["with tf.device('/GPU:0'):\n","    proc_train_df= df_preprocessing(train_matrix,feature_col)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HCmKl_QtZIB_"},"source":["proc_test_df = df_preprocessing(test_matrix,feature_col)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0AurD85DVOD-"},"source":["from transformers import AutoTokenizer,TFDistilBertModel, DistilBertConfig\n","from transformers import TFAutoModel\n","import tensorflow as tf \n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Dropout\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","#import tensorflow_addons as tfa\n","\n","\n","#Creating tokenizer\n","def create_tokenizer(pretrained_weights='distilbert-base-uncased'):\n","  '''Function to create the tokenizer'''\n","\n","  tokenizer = AutoTokenizer.from_pretrained(pretrained_weights)\n","  return tokenizer\n","\n","#Tokenization of the data\n","def data_tokenization(dataset,feature_col,max_len,tokenizer):\n","    '''dataset: Pandas dataframe with feature name is column name \n","    Pretrained_weights: selected model \n","    RETURN: [input_ids, attention_mask]'''\n","\n","    tokens = dataset[feature_col].apply(lambda x: tokenizer(x,return_tensors='tf', \n","                                                            truncation=True,\n","                                                            padding='max_length',\n","                                                            max_length=max_len, \n","                                                            add_special_tokens=True))\n","    input_ids= []\n","    attention_mask=[]\n","    for item in tokens:\n","        input_ids.append(item['input_ids'])\n","        attention_mask.append(item['attention_mask'])\n","    input_ids, attention_mask=np.squeeze(input_ids), np.squeeze(attention_mask)\n","\n","\n","    return [input_ids,attention_mask]\n","\n","def bert_model(pretrained_weights,max_len,learning_rate):\n","  '''BERT model creation with pretrained weights\n","  INPUT:\n","  pretrained_weights: Language model pretrained weights\n","  max_len: input length '''\n","  print('Model selected:', pretrained_weights)\n","  bert=TFAutoModel.from_pretrained(pretrained_weights)\n","  \n","  # This is must if you would like to train the layers of language models too.\n","  for layer in bert.layers:\n","      layer.trainable = True\n","\n","  # parameter declaration\n","#   step = tf.Variable(0, trainable=False)\n","#   schedule = tf.optimizers.schedules.PiecewiseConstantDecay([10000, 15000], [2e-0, 2e-1, 1e-2])\n","#   # lr and wd can be a function or a tensor\n","#   lr = learning_rate * schedule(step)\n","#   wd = lambda:lr * schedule(step)\n","#   optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n","\n","  optimizer= tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n","#   optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","  # declaring inputs, BERT take input_ids and attention_mask as input\n","  input_ids= Input(shape=(max_len,),dtype=tf.int32,name='input_ids')\n","  attention_mask=Input(shape=(max_len,),dtype=tf.int32,name='attention_mask')\n","\n","  bert= bert(input_ids,attention_mask=attention_mask)\n","  x= bert[0][:,0,:]\n","  x=tf.keras.layers.Dropout(0.1)(x)\n","  x= tf.keras.layers.Dense(128)(x)\n","  x=tf.keras.layers.Dense(64)(x)\n","  x=tf.keras.layers.Dense(32)(x)\n","\n","  output=tf.keras.layers.Dense(maxi,activation='sigmoid')(x)\n","\n","  model=Model(inputs=[input_ids,attention_mask],outputs=[output])\n","  # compiling model \n","  model.compile(optimizer=optimizer,\n","                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE,name='binary_crossentropy'),\n","                metrics=['accuracy'])\n","  return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BxVIN9-nWZdF"},"source":["pretrained_weights='bert-base-uncased'\n","max_len=256\n","epochs=20\n","learning_rate=2e-5\n","batch_size=4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r32leLI-WbE3"},"source":["tokenizer= create_tokenizer(pretrained_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xv6g_Tk-WcIX"},"source":["x_train= data_tokenization(proc_train_df,feature_col,max_len,tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1TxmQBYUbVy_"},"source":["y_train= proc_train_df[target_col].values\n","y_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3GZUWIrbbrA"},"source":["bert=bert_model(pretrained_weights,max_len,learning_rate)\n","bert.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WiXYUhBObi3Q"},"source":["with tf.device('/GPU:0'):\n","    bert.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rTNa8ZPfcdmo"},"source":["x_test= data_tokenization(test_matrix,feature_col,max_len,tokenizer)\n","x_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bdfTMIm2cmlQ"},"source":["preds= bert.predict(x_test)\n","submiss_df= pd.DataFrame(preds, columns= target_col)\n","submiss_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YrUWcBYwc5vp"},"source":["outputs = submiss_df>0.5\n","f1_score_micro = metrics.f1_score(test_matrix.iloc[:,1:], outputs, average='micro')\n","f1_score_macro = metrics.f1_score(test_matrix.iloc[:,1:], outputs, average='macro')\n","ap = metrics.average_precision_score(test_matrix.iloc[:,1:], outputs, average='micro')\n","print(f1_score_micro)\n","print(f1_score_macro)\n","print(ap)"],"execution_count":null,"outputs":[]}]}