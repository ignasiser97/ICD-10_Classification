{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"TFM_new_D_English.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"LPK6NhkGUa1S"},"source":["import os\n","import collections\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from datetime import datetime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g2QcRAEoyiAr"},"source":["!pip install -q transformers\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6gX0SwlqyeVH"},"source":["# Importing stock ml libraries\n","import numpy as np\n","import pandas as pd\n","from sklearn import metrics\n","import transformers\n","import torch\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer, BertModel, BertConfig"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLMc17djbndj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629217511059,"user_tz":-120,"elapsed":21,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"d7c57145-6351-4955-f887-d5f9e1e185a8"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8lDUPGD8Ip4K"},"source":["TRAIN D"]},{"cell_type":"code","metadata":{"id":"4Z8jjfw4Io9X"},"source":["import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","df_train_clinical_P = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/train/trainD.tsv\", header=None, sep=\"\\t\")\n","\n","df_train_P = pd.DataFrame()\n","def load_articles(x,i):\n","    path = Path(r'/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/train/text_files_en') / x[0][i]\n","    path = str(path) +'.txt'\n","    df_1 = pd.read_csv(path, header=None, sep=\"\\n\")\n","    df_def = df_1.values[0] \n","    for j in range(1, len(df_1)):\n","        df_def = df_def + df_1.values[j]\n","    return pd.DataFrame(df_def)\n","\n","count= 0\n","for i in range(0, len(df_train_clinical_P)):\n","    df_2 = load_articles(df_train_clinical_P, i)\n","    df_train_P = df_train_P.append(df_2)\n","    count = count + 1\n","\n","\n","df_train_P = df_train_P.set_axis(np.array(range(0,len(df_train_clinical_P))))\n","df_train_P[1] = df_train_clinical_P[1]\n","df_train_P.columns = ['content', 'labels']\n","df_train_P.head(10)\n","\n","train = df_train_P"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SD5EFsIh6wqI"},"source":["DEV D"]},{"cell_type":"code","metadata":{"id":"BMu-J1vE6y5K"},"source":["import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","df_dev_clinical_P = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/dev/devD.tsv\", header=None, sep=\"\\t\")\n","\n","df_dev_P = pd.DataFrame()\n","def load_articles(x,i):\n","    path = Path(r'/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/dev/text_files_en') / x[0][i]\n","    path = str(path) +'.txt'\n","    df_1 = pd.read_csv(path, header=None, sep=\"\\n\")\n","    df_def = df_1.values[0] \n","    for j in range(1, len(df_1)):\n","        df_def = df_def + df_1.values[j]\n","    return pd.DataFrame(df_def)\n","\n","count= 0\n","for i in range(0, len(df_dev_clinical_P)):\n","    df_2 = load_articles(df_dev_clinical_P, i)\n","    df_dev_P = df_dev_P.append(df_2)\n","    count = count + 1\n","\n","\n","df_dev_P = df_dev_P.set_axis(np.array(range(0,len(df_dev_clinical_P))))\n","df_dev_P[1] = df_dev_clinical_P[1]\n","df_dev_P.columns = ['content', 'labels']\n","df_dev_P.head(10)\n","\n","dev = df_dev_P"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v6Igbf42JTY4"},"source":["TEST D"]},{"cell_type":"code","metadata":{"id":"V8XsgblOJUWY"},"source":["from pathlib import Path\n","df_test_clinical_P = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/test/testD.tsv\", header=None, sep=\"\\t\")\n","\n","df_test_P = pd.DataFrame()\n","def load_articles(x,i):\n","    path = Path(r'/content/drive/MyDrive/Colab Notebooks/final_dataset_v4_to_publish/test/text_files_en') / x[0][i]\n","    path = str(path) +'.txt'\n","    df_1 = pd.read_csv(path, header=None, sep=\"\\n\", dtype='unicode', error_bad_lines=False)\n","    df_def = df_1.values[0] \n","    for j in range(1, len(df_1)):\n","        df_def = df_def + df_1.values[j]\n","    return pd.DataFrame(df_def)\n","\n","count= 0\n","for i in range(0, len(df_test_clinical_P)):\n","    df_2 = load_articles(df_test_clinical_P, i)\n","    df_test_P = df_test_P.append(df_2)\n","    count = count + 1\n","\n","\n","df_test_P = df_test_P.set_axis(np.array(range(0,len(df_test_clinical_P))))\n","df_test_P[1] = df_test_clinical_P[1]\n","LABEL_COLUMNS = ['content', 'labels']\n","df_test_P.columns = LABEL_COLUMNS\n","df_test_P.head(10)\n","\n","test = df_test_P"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tIfFAifwTWSw"},"source":["**MATRIX TRANSFORMATION**"]},{"cell_type":"code","metadata":{"id":"zCW8KdFrTVqJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629217621009,"user_tz":-120,"elapsed":76932,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"f23435a6-86fe-476a-ea4b-c72a921b2975"},"source":["new_labels_train = []\n","new_labels_test = []\n","new_labels_dev = []\n","lista = ['content']\n","for i in range(len(df_train_P['labels'])):\n","    new_labels_train.append([df_train_P['labels'][i]])\n","    lista = lista + new_labels_train[i]\n","for i in range(len(df_test_P['labels'])):\n","    new_labels_test.append([df_test_P['labels'][i]])\n","    lista = lista + new_labels_test[i]\n","for i in range(len(df_dev_P['labels'])):\n","    new_labels_dev.append([df_dev_P['labels'][i]])\n","    lista = lista + new_labels_dev[i]\n","\n","    \n","####### TRAIN #######\n","myvec = np.zeros((len(df_train_P.drop_duplicates('content', ignore_index = True)),len(lista)))\n","auxdf = pd.DataFrame(data = myvec, index=range(len(df_train_P.drop_duplicates('content', ignore_index = True))),columns=range(len(lista)))\n","auxdf.columns = lista\n","auxdf = auxdf.loc[:,~auxdf.columns.duplicated()]\n","\n","df_train_P1 = df_train_P\n","df_train_P = df_train_P.drop_duplicates('content', ignore_index = True)\n","auxdf['content'] = df_train_P['content']\n","\n","for j in range(len(auxdf['content'])):\n","    for i in range(len(df_train_P1['content'])):\n","        label = df_train_P1['labels'][i]\n","        if auxdf['content'][j]==df_train_P1['content'][i]:\n","            label = df_train_P1['labels'][i]\n","            auxdf[label][j] = 1 \n","            \n","train_matrix = auxdf            \n","\n","\n","#### TEST #####\n","myvec1 = np.zeros((len(df_test_P.drop_duplicates('content', ignore_index = True)),len(lista)))\n","auxdf1 = pd.DataFrame(data = myvec1, index=range(len(df_test_P.drop_duplicates('content', ignore_index = True))),columns=range(len(lista)))\n","auxdf1.columns = lista\n","auxdf1 = auxdf1.loc[:,~auxdf1.columns.duplicated()]\n","\n","df_test_P1 = df_test_P\n","df_test_P = df_test_P.drop_duplicates('content', ignore_index = True)\n","auxdf1['content'] = df_test_P['content']\n","\n","for j in range(len(auxdf1['content'])):\n","    for i in range(len(df_test_P1['content'])):\n","        label1 = df_test_P1['labels'][i]\n","        if auxdf1['content'][j]==df_test_P1['content'][i]:\n","            label1 = df_test_P1['labels'][i]\n","            auxdf1[label1][j] = 1 \n","            \n","test_matrix = auxdf1            \n","\n","\n","###### DEV ######\n","myvec3 = np.zeros((len(df_dev_P.drop_duplicates('content', ignore_index = True)),len(lista)))\n","auxdf3 = pd.DataFrame(data = myvec3, index=range(len(df_dev_P.drop_duplicates('content', ignore_index = True))),columns=range(len(lista)))\n","auxdf3.columns = lista\n","auxdf3 = auxdf3.loc[:,~auxdf3.columns.duplicated()]\n","\n","df_dev_P1 = df_dev_P\n","df_dev_P = df_dev_P.drop_duplicates('content', ignore_index = True)\n","auxdf3['content'] = df_dev_P['content']\n","\n","for j in range(len(auxdf3['content'])):\n","    for i in range(len(df_dev_P1['content'])):\n","        label3 = df_dev_P1['labels'][i]\n","        if auxdf3['content'][j]==df_dev_P1['content'][i]:\n","            label = df_dev_P1['labels'][i]\n","            auxdf3[label3][j] = 1 \n","            \n","dev_matrix = auxdf3            \n","\n","\n","##### TRAIN MATRIX #####\n","df_train_clinical = pd.DataFrame()\n","# train_matrix = pd.concat([train_matrix, dev_matrix], ignore_index=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"},{"output_type":"stream","text":["                                               content  n44.8  ...  d13.5  n81.2\n","0    We describe the case of a 37-year-old man with...    1.0  ...    0.0    0.0\n","1    We report the case of a 29-year-old woman who ...    0.0  ...    0.0    0.0\n","2    A 36-year-old male, with no relevant past medi...    0.0  ...    0.0    0.0\n","3    A 42-year-old woman underwent liver transplant...    0.0  ...    0.0    0.0\n","4    A 65-year-old male presented with an infravesi...    0.0  ...    0.0    0.0\n","..                                                 ...    ...  ...    ...    ...\n","495  We describe the case of a 47-year-old female p...    0.0  ...    0.0    0.0\n","496  A 65-year-old male patient diagnosed with OI f...    0.0  ...    0.0    0.0\n","497  A 72-year-old male in HD program from December...    0.0  ...    0.0    0.0\n","498  A 33-year-old woman presented to the emergency...    0.0  ...    0.0    0.0\n","499  We report the case of a 62-year-old patient wh...    0.0  ...    0.0    0.0\n","\n","[500 rows x 2558 columns]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"},{"output_type":"stream","text":["                                               content  n44.8  ...  d13.5  n81.2\n","0    Patient of 70 years old, retired miner, withou...    0.0  ...    0.0    0.0\n","1    A seventeen-year-old male with no history of i...    0.0  ...    0.0    0.0\n","2    A 62-year-old female patient, obese, with diab...    0.0  ...    0.0    0.0\n","3    A 53-year-old man with no relevant past medica...    0.0  ...    0.0    0.0\n","4    Male patient, 40 years old, with a history of ...    0.0  ...    0.0    0.0\n","..                                                 ...    ...  ...    ...    ...\n","245  The case described in our study is that of a 7...    0.0  ...    0.0    0.0\n","246  22-year-old man weight, 172 cm.), active milit...    0.0  ...    0.0    0.0\n","247  A 53-year-old male, road veteran cyclist, with...    0.0  ...    0.0    0.0\n","248  A 34-year-old male presented with a two-week h...    0.0  ...    0.0    0.0\n","249  A 57-year-old male with a history of arterial ...    0.0  ...    0.0    0.0\n","\n","[250 rows x 2558 columns]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:73: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"},{"output_type":"stream","text":["                                               content  n44.8  ...  d13.5  n81.2\n","0    A 29-year-old woman with a history of duodenal...    0.0  ...    0.0    0.0\n","1    58-year-old male at the time of transplantatio...    0.0  ...    0.0    0.0\n","2    A 22-year-old male patient, with no history of...    0.0  ...    0.0    0.0\n","3    A 35-year-old patient was admitted to our emer...    0.0  ...    0.0    0.0\n","4    A 90-year-old patient came to his urologist in...    0.0  ...    0.0    0.0\n","..                                                 ...    ...  ...    ...    ...\n","245  A five-year-old male patient who after sufferi...    0.0  ...    0.0    0.0\n","246  This is a 16-year-old adolescent, basketball p...    0.0  ...    0.0    0.0\n","247  We describe the case of a 58-year-old woman.Sh...    0.0  ...    0.0    0.0\n","248  We report the case of a 64-year-old male with ...    0.0  ...    0.0    0.0\n","249  A 73-year-old woman with a history of hyperten...    0.0  ...    1.0    1.0\n","\n","[250 rows x 2558 columns]\n","                                               content  n44.8  ...  d13.5  n81.2\n","0    We describe the case of a 37-year-old man with...    1.0  ...    0.0    0.0\n","1    We report the case of a 29-year-old woman who ...    0.0  ...    0.0    0.0\n","2    A 36-year-old male, with no relevant past medi...    0.0  ...    0.0    0.0\n","3    A 42-year-old woman underwent liver transplant...    0.0  ...    0.0    0.0\n","4    A 65-year-old male presented with an infravesi...    0.0  ...    0.0    0.0\n","..                                                 ...    ...  ...    ...    ...\n","495  We describe the case of a 47-year-old female p...    0.0  ...    0.0    0.0\n","496  A 65-year-old male patient diagnosed with OI f...    0.0  ...    0.0    0.0\n","497  A 72-year-old male in HD program from December...    0.0  ...    0.0    0.0\n","498  A 33-year-old woman presented to the emergency...    0.0  ...    0.0    0.0\n","499  We report the case of a 62-year-old patient wh...    0.0  ...    0.0    0.0\n","\n","[500 rows x 2558 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fEx_ZbiPunA_"},"source":["train_matrix1 = train_matrix\n","test_matrix1 = test_matrix\n","dev_matrix1 = dev_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WiDnOJk4ucV3"},"source":["total = []\n","for i in range(train_matrix1.shape[1]):\n","    if i>0:\n","        total.append(sum(train_matrix1.iloc[:,i]))\n","\n","\n","total_matrix = pd.concat([train_matrix1, test_matrix1, dev_matrix1], ignore_index=True)\n","lista = total_matrix.columns\n","lista = lista.delete(0)\n","\n","mat = pd.DataFrame(total, index = lista)\n","mat = mat.sort_values(0, ascending = False)\n","\n","#maxi = len(lista) \n","maxi = 161\n","\n","coger = mat.iloc[:maxi]\n","coger = coger.index\n","coger = coger.insert(0,'content')\n","\n","train_matrix = train_matrix[coger]\n","test_matrix = test_matrix[coger]\n","dev_matrix = dev_matrix[coger]\n","# print(train_matrix)\n","# print(test_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EVPN0RkqXAHP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629217626825,"user_tz":-120,"elapsed":5825,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"ec09adc7-c659-4496-c533-9e877edc105c"},"source":["!pip install texthero\n","!pip install tweet-preprocessor"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: texthero in /usr/local/lib/python3.7/dist-packages (1.1.0)\n","Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from texthero) (0.22.2.post1)\n","Requirement already satisfied: wordcloud>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.5.0)\n","Requirement already satisfied: unidecode>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.2.0)\n","Requirement already satisfied: pandas>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.1.5)\n","Requirement already satisfied: spacy<3.0.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (2.2.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.19.5)\n","Requirement already satisfied: nltk>=3.3 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.6.2)\n","Requirement already satisfied: plotly>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.4.1)\n","Requirement already satisfied: gensim<4.0,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.6.0)\n","Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.2.2)\n","Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.62.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.4.1)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (5.1.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.15.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (1.3.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (2019.12.20)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (1.0.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.2->texthero) (2018.9)\n","Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.2.0->texthero) (1.3.3)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (7.4.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.0.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.4.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (57.4.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.23.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (3.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.8.2)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.1.3)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (4.6.3)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (3.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.10)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud>=1.5.0->texthero) (7.1.2)\n","Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.7/dist-packages (0.6.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r0SNgblYW9xI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629217628524,"user_tz":-120,"elapsed":1708,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"075b4441-8f10-47a9-ef54-84ae6485f680"},"source":["import re \n","import nltk\n","from wordcloud import WordCloud\n","from nltk.stem import WordNetLemmatizer \n","from textblob import TextBlob,Word\n","from nltk.corpus import words\n","nltk.download('words')\n","import texthero as hero\n","import re\n","from texthero import stopwords\n","\n","from nltk.corpus import wordnet\n","\n","import tensorflow as tf\n","\n","from nltk.corpus import stopwords\n","from nltk.tokenize import TweetTokenizer\n","\n","import tensorflow as tf\n","\n","\n","def lemma_per_pos(sent):\n","    '''function to lemmatize according to part of speech tag'''\n","    tweet_tokenizer=TweetTokenizer()\n","    lemmatizer = nltk.stem.WordNetLemmatizer()\n","    lemmatized_list = [lemmatizer.lemmatize(w) for w in  tweet_tokenizer.tokenize(sent)]\n","    return \" \".join(lemmatized_list)\n","\n","def df_preprocessing(df,feature_col):\n","    '''\n","    Preprocessing of dataframe\n","    '''\n","    stop = set(stopwords.words('english'))\n","    df[feature_col]= (df[feature_col].pipe(hero.lowercase).\n","                      pipe(hero.remove_urls).\n","                      pipe(hero.remove_digits).\n","                      pipe(hero.remove_punctuation).\n","                      pipe(hero.remove_html_tags) )\n","    # lemmatization\n","#     df[feature_col]= [lemma_per_pos(sent) for sent in df[feature_col]]\n","    # df[col_name]= hero.remove_stopwords(df[col_name],custom_stopwords)\n","    return df"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"qKmc9oqiYtdY"},"source":["target_col= train_matrix.columns[1:]\n","feature_col= train_matrix.columns[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8KbteS5WJVQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629217629422,"user_tz":-120,"elapsed":901,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"17cb808b-e1df-45cc-bb41-05e2475ddaf3"},"source":["with tf.device('/GPU:0'):\n","    proc_train_df= df_preprocessing(train_matrix,feature_col)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: SettingWithCopyWarning:\n","\n","\n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"HCmKl_QtZIB_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629217629423,"user_tz":-120,"elapsed":14,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"1fad44a4-9875-4979-b700-2415d2b0d209"},"source":["proc_test_df = df_preprocessing(test_matrix,feature_col)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: SettingWithCopyWarning:\n","\n","\n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"0AurD85DVOD-"},"source":["from transformers import AutoTokenizer,TFDistilBertModel, DistilBertConfig\n","from transformers import TFAutoModel\n","import tensorflow as tf \n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Dropout\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","#import tensorflow_addons as tfa\n","\n","\n","#Creating tokenizer\n","def create_tokenizer(pretrained_weights='distilbert-base-uncased'):\n","  '''Function to create the tokenizer'''\n","\n","  tokenizer = AutoTokenizer.from_pretrained(pretrained_weights)\n","  return tokenizer\n","\n","#Tokenization of the data\n","def data_tokenization(dataset,feature_col,max_len,tokenizer):\n","    '''dataset: Pandas dataframe with feature name is column name \n","    Pretrained_weights: selected model \n","    RETURN: [input_ids, attention_mask]'''\n","\n","    tokens = dataset[feature_col].apply(lambda x: tokenizer(x,return_tensors='tf', \n","                                                            truncation=True,\n","                                                            padding='max_length',\n","                                                            max_length=max_len, \n","                                                            add_special_tokens=True))\n","    input_ids= []\n","    attention_mask=[]\n","    for item in tokens:\n","        input_ids.append(item['input_ids'])\n","        attention_mask.append(item['attention_mask'])\n","    input_ids, attention_mask=np.squeeze(input_ids), np.squeeze(attention_mask)\n","\n","\n","    return [input_ids,attention_mask]\n","\n","def bert_model(pretrained_weights,max_len,learning_rate):\n","  '''BERT model creation with pretrained weights\n","  INPUT:\n","  pretrained_weights: Language model pretrained weights\n","  max_len: input length '''\n","  print('Model selected:', pretrained_weights)\n","  bert=TFAutoModel.from_pretrained(pretrained_weights)\n","  \n","  # This is must if you would like to train the layers of language models too.\n","  for layer in bert.layers:\n","      layer.trainable = True\n","\n","  # parameter declaration\n","#   step = tf.Variable(0, trainable=False)\n","#   schedule = tf.optimizers.schedules.PiecewiseConstantDecay([10000, 15000], [2e-0, 2e-1, 1e-2])\n","#   # lr and wd can be a function or a tensor\n","#   lr = learning_rate * schedule(step)\n","#   wd = lambda:lr * schedule(step)\n","#   optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n","\n","  optimizer= tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n","#   optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","  # declaring inputs, BERT take input_ids and attention_mask as input\n","  input_ids= Input(shape=(max_len,),dtype=tf.int32,name='input_ids')\n","  attention_mask=Input(shape=(max_len,),dtype=tf.int32,name='attention_mask')\n","\n","  bert= bert(input_ids,attention_mask=attention_mask)\n","  x= bert[0][:,0,:]\n","  x=tf.keras.layers.Dropout(0.1)(x)\n","  x= tf.keras.layers.Dense(128)(x)\n","  x=tf.keras.layers.Dense(64)(x)\n","  x=tf.keras.layers.Dense(32)(x)\n","\n","  output=tf.keras.layers.Dense(maxi,activation='sigmoid')(x)\n","\n","  model=Model(inputs=[input_ids,attention_mask],outputs=[output])\n","  # compiling model \n","  model.compile(optimizer=optimizer,\n","                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE,name='binary_crossentropy'),\n","                metrics=['accuracy'])\n","  return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BxVIN9-nWZdF"},"source":["pretrained_weights='bert-base-uncased'\n","max_len=256\n","epochs=20\n","learning_rate=2e-5\n","batch_size=4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r32leLI-WbE3"},"source":["tokenizer= create_tokenizer(pretrained_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xv6g_Tk-WcIX"},"source":["x_train= data_tokenization(proc_train_df,feature_col,max_len,tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1TxmQBYUbVy_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629217632864,"user_tz":-120,"elapsed":16,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"2972f22d-4cdf-4c91-b336-0b1020f55653"},"source":["y_train= proc_train_df[target_col].values\n","y_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1., 0., 1., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [1., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 1., 0., ..., 0., 0., 0.],\n","       [0., 1., 0., ..., 0., 0., 0.]])"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"c3GZUWIrbbrA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629217670459,"user_tz":-120,"elapsed":37607,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"08856dd5-13fb-4f9d-d313-e29bdf4c1429"},"source":["bert=bert_model(pretrained_weights,max_len,learning_rate)\n","bert.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model selected: bert-base-uncased\n"],"name":"stdout"},{"output_type":"stream","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f498df1bf30>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f498df1bf30>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f49a90cd9e0> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING: AutoGraph could not transform <function wrap at 0x7f49a90cd9e0> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n","Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_ids (InputLayer)          [(None, 256)]        0                                            \n","__________________________________________________________________________________________________\n","attention_mask (InputLayer)     [(None, 256)]        0                                            \n","__________________________________________________________________________________________________\n","tf_bert_model (TFBertModel)     TFBaseModelOutputWit 109482240   input_ids[0][0]                  \n","                                                                 attention_mask[0][0]             \n","__________________________________________________________________________________________________\n","tf.__operators__.getitem (Slici (None, 768)          0           tf_bert_model[0][0]              \n","__________________________________________________________________________________________________\n","dropout_37 (Dropout)            (None, 768)          0           tf.__operators__.getitem[0][0]   \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 128)          98432       dropout_37[0][0]                 \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 64)           8256        dense[0][0]                      \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 32)           2080        dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 161)          5313        dense_2[0][0]                    \n","==================================================================================================\n","Total params: 109,596,321\n","Trainable params: 109,596,321\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WiXYUhBObi3Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629218296246,"user_tz":-120,"elapsed":625818,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"57ac91cf-2f8e-472e-bfc2-d285267bd4f4"},"source":["with tf.device('/GPU:0'):\n","    bert.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,verbose=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:5017: UserWarning:\n","\n","\"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n","125/125 [==============================] - 36s 224ms/step - loss: 0.5730 - accuracy: 0.0000e+00\n","Epoch 2/20\n","125/125 [==============================] - 29s 230ms/step - loss: 0.4986 - accuracy: 0.0000e+00\n","Epoch 3/20\n","125/125 [==============================] - 30s 237ms/step - loss: 0.4224 - accuracy: 0.0000e+00\n","Epoch 4/20\n","125/125 [==============================] - 30s 242ms/step - loss: 0.3461 - accuracy: 0.0040\n","Epoch 5/20\n","125/125 [==============================] - 31s 246ms/step - loss: 0.2789 - accuracy: 0.0020\n","Epoch 6/20\n","125/125 [==============================] - 31s 251ms/step - loss: 0.2247 - accuracy: 0.0060\n","Epoch 7/20\n","125/125 [==============================] - 31s 251ms/step - loss: 0.1874 - accuracy: 0.0180\n","Epoch 8/20\n","125/125 [==============================] - 31s 250ms/step - loss: 0.1629 - accuracy: 0.0680\n","Epoch 9/20\n","125/125 [==============================] - 31s 250ms/step - loss: 0.1468 - accuracy: 0.1580\n","Epoch 10/20\n","125/125 [==============================] - 31s 251ms/step - loss: 0.1341 - accuracy: 0.2600\n","Epoch 11/20\n","125/125 [==============================] - 31s 251ms/step - loss: 0.1241 - accuracy: 0.3240\n","Epoch 12/20\n","125/125 [==============================] - 31s 251ms/step - loss: 0.1162 - accuracy: 0.3800\n","Epoch 13/20\n","125/125 [==============================] - 31s 251ms/step - loss: 0.1094 - accuracy: 0.3980\n","Epoch 14/20\n","125/125 [==============================] - 31s 250ms/step - loss: 0.1036 - accuracy: 0.4180\n","Epoch 15/20\n","125/125 [==============================] - 31s 250ms/step - loss: 0.0993 - accuracy: 0.4740\n","Epoch 16/20\n","125/125 [==============================] - 31s 251ms/step - loss: 0.0945 - accuracy: 0.4880\n","Epoch 17/20\n","125/125 [==============================] - 31s 251ms/step - loss: 0.0908 - accuracy: 0.4560\n","Epoch 18/20\n","125/125 [==============================] - 31s 251ms/step - loss: 0.0875 - accuracy: 0.4800\n","Epoch 19/20\n","125/125 [==============================] - 31s 251ms/step - loss: 0.0847 - accuracy: 0.5000\n","Epoch 20/20\n","125/125 [==============================] - 31s 251ms/step - loss: 0.0818 - accuracy: 0.4720\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rTNa8ZPfcdmo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629218296609,"user_tz":-120,"elapsed":394,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"80f6ee99-3827-441b-a544-618a3521c4a1"},"source":["x_test= data_tokenization(test_matrix,feature_col,max_len,tokenizer)\n","x_test"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([[  101,  5776,  1997, ...,  2015,  2024,   102],\n","        [  101,  1037,  9171, ...,     0,     0,     0],\n","        [  101,  1037,  2095, ..., 25320,  2007,   102],\n","        ...,\n","        [  101,  1037,  2095, ..., 22260,  9386,   102],\n","        [  101,  1037,  2095, ...,  5970,  1996,   102],\n","        [  101,  1037,  2095, ...,  2659,  2067,   102]], dtype=int32),\n"," array([[1, 1, 1, ..., 1, 1, 1],\n","        [1, 1, 1, ..., 0, 0, 0],\n","        [1, 1, 1, ..., 1, 1, 1],\n","        ...,\n","        [1, 1, 1, ..., 1, 1, 1],\n","        [1, 1, 1, ..., 1, 1, 1],\n","        [1, 1, 1, ..., 1, 1, 1]], dtype=int32)]"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"bdfTMIm2cmlQ","colab":{"base_uri":"https://localhost:8080/","height":643},"executionInfo":{"status":"ok","timestamp":1629218303914,"user_tz":-120,"elapsed":7314,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"07d2837b-030f-417e-c81a-f530246b507f"},"source":["preds= bert.predict(x_test)\n","submiss_df= pd.DataFrame(preds, columns= target_col)\n","submiss_df"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>r52</th>\n","      <th>r69</th>\n","      <th>r50.9</th>\n","      <th>i10</th>\n","      <th>r60.9</th>\n","      <th>r59.9</th>\n","      <th>r53.1</th>\n","      <th>r11.10</th>\n","      <th>r59.0</th>\n","      <th>r10.9</th>\n","      <th>b99.9</th>\n","      <th>r58</th>\n","      <th>d64.9</th>\n","      <th>e11.9</th>\n","      <th>i96</th>\n","      <th>r63.4</th>\n","      <th>r31.9</th>\n","      <th>l53.9</th>\n","      <th>b20</th>\n","      <th>n28.9</th>\n","      <th>r23.1</th>\n","      <th>d72.829</th>\n","      <th>f17.210</th>\n","      <th>t14.90</th>\n","      <th>r51</th>\n","      <th>b19.20</th>\n","      <th>t14.8</th>\n","      <th>r06.00</th>\n","      <th>k65.9</th>\n","      <th>n28.89</th>\n","      <th>l98.9</th>\n","      <th>k75.9</th>\n","      <th>r63.0</th>\n","      <th>n18.9</th>\n","      <th>l90.5</th>\n","      <th>i82.90</th>\n","      <th>r19.7</th>\n","      <th>e80.7</th>\n","      <th>b19.10</th>\n","      <th>r60.0</th>\n","      <th>...</th>\n","      <th>k13.79</th>\n","      <th>r55</th>\n","      <th>r25.1</th>\n","      <th>n32.9</th>\n","      <th>k56.60</th>\n","      <th>c67.9</th>\n","      <th>a53.9</th>\n","      <th>h26.9</th>\n","      <th>h55.00</th>\n","      <th>r21</th>\n","      <th>k85.90</th>\n","      <th>d72.0</th>\n","      <th>z51.5</th>\n","      <th>h57.04</th>\n","      <th>g83.9</th>\n","      <th>r34</th>\n","      <th>f10.10</th>\n","      <th>k92.1</th>\n","      <th>l81.9</th>\n","      <th>r65.21</th>\n","      <th>d49.2</th>\n","      <th>k63.1</th>\n","      <th>e79.0</th>\n","      <th>b96.20</th>\n","      <th>l92.9</th>\n","      <th>n50.89</th>\n","      <th>h53.8</th>\n","      <th>e46</th>\n","      <th>d49.4</th>\n","      <th>h05.20</th>\n","      <th>r29.2</th>\n","      <th>e87.2</th>\n","      <th>g40.409</th>\n","      <th>i77.6</th>\n","      <th>m81.0</th>\n","      <th>k51.90</th>\n","      <th>i85.00</th>\n","      <th>i50.9</th>\n","      <th>r31.29</th>\n","      <th>r68.89</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.044488</td>\n","      <td>0.525451</td>\n","      <td>0.053205</td>\n","      <td>0.023632</td>\n","      <td>0.008357</td>\n","      <td>0.021558</td>\n","      <td>0.118744</td>\n","      <td>0.009170</td>\n","      <td>0.046695</td>\n","      <td>0.077372</td>\n","      <td>0.035429</td>\n","      <td>0.053127</td>\n","      <td>0.067940</td>\n","      <td>0.016327</td>\n","      <td>0.027141</td>\n","      <td>0.069421</td>\n","      <td>0.168263</td>\n","      <td>0.000269</td>\n","      <td>0.003080</td>\n","      <td>0.044579</td>\n","      <td>0.028184</td>\n","      <td>0.019590</td>\n","      <td>0.327390</td>\n","      <td>0.001245</td>\n","      <td>0.015620</td>\n","      <td>0.000836</td>\n","      <td>0.005496</td>\n","      <td>0.197049</td>\n","      <td>0.017823</td>\n","      <td>0.111516</td>\n","      <td>0.007177</td>\n","      <td>0.006755</td>\n","      <td>0.060079</td>\n","      <td>0.004170</td>\n","      <td>0.042306</td>\n","      <td>0.004898</td>\n","      <td>0.031556</td>\n","      <td>0.109115</td>\n","      <td>0.001776</td>\n","      <td>0.014701</td>\n","      <td>...</td>\n","      <td>0.008321</td>\n","      <td>0.004164</td>\n","      <td>0.002737</td>\n","      <td>0.050898</td>\n","      <td>0.000335</td>\n","      <td>0.003280</td>\n","      <td>0.004258</td>\n","      <td>0.002148</td>\n","      <td>0.000136</td>\n","      <td>0.002933</td>\n","      <td>0.012103</td>\n","      <td>4.324492e-06</td>\n","      <td>0.004402</td>\n","      <td>0.001677</td>\n","      <td>0.016620</td>\n","      <td>0.002962</td>\n","      <td>0.008510</td>\n","      <td>0.002820</td>\n","      <td>0.024794</td>\n","      <td>0.005962</td>\n","      <td>0.001050</td>\n","      <td>0.003250</td>\n","      <td>0.000351</td>\n","      <td>0.000448</td>\n","      <td>0.054232</td>\n","      <td>0.013705</td>\n","      <td>0.008206</td>\n","      <td>0.001036</td>\n","      <td>0.001391</td>\n","      <td>0.001650</td>\n","      <td>0.000441</td>\n","      <td>0.018519</td>\n","      <td>0.004531</td>\n","      <td>0.002215</td>\n","      <td>0.000277</td>\n","      <td>0.020696</td>\n","      <td>4.426308e-04</td>\n","      <td>0.008127</td>\n","      <td>0.023733</td>\n","      <td>0.039619</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.221252</td>\n","      <td>0.333275</td>\n","      <td>0.045961</td>\n","      <td>0.023861</td>\n","      <td>0.002381</td>\n","      <td>0.046451</td>\n","      <td>0.116458</td>\n","      <td>0.002240</td>\n","      <td>0.014030</td>\n","      <td>0.073951</td>\n","      <td>0.029136</td>\n","      <td>0.034962</td>\n","      <td>0.016073</td>\n","      <td>0.003143</td>\n","      <td>0.028669</td>\n","      <td>0.017142</td>\n","      <td>0.219258</td>\n","      <td>0.000367</td>\n","      <td>0.006184</td>\n","      <td>0.016272</td>\n","      <td>0.003124</td>\n","      <td>0.080518</td>\n","      <td>0.013927</td>\n","      <td>0.003336</td>\n","      <td>0.005812</td>\n","      <td>0.000602</td>\n","      <td>0.000153</td>\n","      <td>0.015048</td>\n","      <td>0.001745</td>\n","      <td>0.034811</td>\n","      <td>0.044869</td>\n","      <td>0.001756</td>\n","      <td>0.008225</td>\n","      <td>0.025078</td>\n","      <td>0.012528</td>\n","      <td>0.007204</td>\n","      <td>0.009483</td>\n","      <td>0.003264</td>\n","      <td>0.001828</td>\n","      <td>0.003119</td>\n","      <td>...</td>\n","      <td>0.001512</td>\n","      <td>0.003186</td>\n","      <td>0.001082</td>\n","      <td>0.031582</td>\n","      <td>0.000046</td>\n","      <td>0.000248</td>\n","      <td>0.000149</td>\n","      <td>0.000171</td>\n","      <td>0.001954</td>\n","      <td>0.000464</td>\n","      <td>0.002431</td>\n","      <td>8.994632e-07</td>\n","      <td>0.013210</td>\n","      <td>0.001099</td>\n","      <td>0.004929</td>\n","      <td>0.001440</td>\n","      <td>0.003877</td>\n","      <td>0.001791</td>\n","      <td>0.004221</td>\n","      <td>0.004480</td>\n","      <td>0.000581</td>\n","      <td>0.000572</td>\n","      <td>0.000409</td>\n","      <td>0.001000</td>\n","      <td>0.001352</td>\n","      <td>0.006096</td>\n","      <td>0.000382</td>\n","      <td>0.000086</td>\n","      <td>0.009162</td>\n","      <td>0.001577</td>\n","      <td>0.000141</td>\n","      <td>0.019106</td>\n","      <td>0.001412</td>\n","      <td>0.004552</td>\n","      <td>0.000020</td>\n","      <td>0.011536</td>\n","      <td>2.083620e-05</td>\n","      <td>0.000972</td>\n","      <td>0.006062</td>\n","      <td>0.006854</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.102241</td>\n","      <td>0.084894</td>\n","      <td>0.105460</td>\n","      <td>0.689151</td>\n","      <td>0.219120</td>\n","      <td>0.043866</td>\n","      <td>0.016247</td>\n","      <td>0.026182</td>\n","      <td>0.032868</td>\n","      <td>0.028080</td>\n","      <td>0.151873</td>\n","      <td>0.085986</td>\n","      <td>0.176472</td>\n","      <td>0.478261</td>\n","      <td>0.032855</td>\n","      <td>0.023018</td>\n","      <td>0.026488</td>\n","      <td>0.000265</td>\n","      <td>0.010286</td>\n","      <td>0.009743</td>\n","      <td>0.028849</td>\n","      <td>0.005962</td>\n","      <td>0.018375</td>\n","      <td>0.003329</td>\n","      <td>0.008142</td>\n","      <td>0.001510</td>\n","      <td>0.028904</td>\n","      <td>0.039816</td>\n","      <td>0.007149</td>\n","      <td>0.015634</td>\n","      <td>0.038553</td>\n","      <td>0.002165</td>\n","      <td>0.003237</td>\n","      <td>0.025476</td>\n","      <td>0.000268</td>\n","      <td>0.012191</td>\n","      <td>0.003968</td>\n","      <td>0.201428</td>\n","      <td>0.016455</td>\n","      <td>0.044299</td>\n","      <td>...</td>\n","      <td>0.003473</td>\n","      <td>0.023257</td>\n","      <td>0.008901</td>\n","      <td>0.000397</td>\n","      <td>0.000617</td>\n","      <td>0.015700</td>\n","      <td>0.019693</td>\n","      <td>0.004106</td>\n","      <td>0.000959</td>\n","      <td>0.004273</td>\n","      <td>0.009729</td>\n","      <td>1.540067e-04</td>\n","      <td>0.004300</td>\n","      <td>0.000829</td>\n","      <td>0.004076</td>\n","      <td>0.002725</td>\n","      <td>0.000151</td>\n","      <td>0.007331</td>\n","      <td>0.016885</td>\n","      <td>0.001775</td>\n","      <td>0.000351</td>\n","      <td>0.002425</td>\n","      <td>0.000089</td>\n","      <td>0.015428</td>\n","      <td>0.001158</td>\n","      <td>0.000075</td>\n","      <td>0.005628</td>\n","      <td>0.007055</td>\n","      <td>0.003255</td>\n","      <td>0.016407</td>\n","      <td>0.002212</td>\n","      <td>0.001681</td>\n","      <td>0.003232</td>\n","      <td>0.003341</td>\n","      <td>0.000044</td>\n","      <td>0.000370</td>\n","      <td>8.678510e-05</td>\n","      <td>0.125863</td>\n","      <td>0.070329</td>\n","      <td>0.000314</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.104872</td>\n","      <td>0.394633</td>\n","      <td>0.043287</td>\n","      <td>0.022177</td>\n","      <td>0.002260</td>\n","      <td>0.064129</td>\n","      <td>0.082529</td>\n","      <td>0.008852</td>\n","      <td>0.088264</td>\n","      <td>0.141014</td>\n","      <td>0.024893</td>\n","      <td>0.052414</td>\n","      <td>0.377239</td>\n","      <td>0.007802</td>\n","      <td>0.074676</td>\n","      <td>0.033781</td>\n","      <td>0.100224</td>\n","      <td>0.000531</td>\n","      <td>0.004695</td>\n","      <td>0.024215</td>\n","      <td>0.008249</td>\n","      <td>0.007194</td>\n","      <td>0.163092</td>\n","      <td>0.000875</td>\n","      <td>0.001915</td>\n","      <td>0.000524</td>\n","      <td>0.008580</td>\n","      <td>0.047065</td>\n","      <td>0.018278</td>\n","      <td>0.177543</td>\n","      <td>0.011990</td>\n","      <td>0.001229</td>\n","      <td>0.006593</td>\n","      <td>0.008161</td>\n","      <td>0.035344</td>\n","      <td>0.003179</td>\n","      <td>0.001518</td>\n","      <td>0.044769</td>\n","      <td>0.000517</td>\n","      <td>0.019482</td>\n","      <td>...</td>\n","      <td>0.006270</td>\n","      <td>0.139042</td>\n","      <td>0.005387</td>\n","      <td>0.005729</td>\n","      <td>0.000204</td>\n","      <td>0.001488</td>\n","      <td>0.000641</td>\n","      <td>0.000839</td>\n","      <td>0.000448</td>\n","      <td>0.018065</td>\n","      <td>0.014170</td>\n","      <td>9.094298e-07</td>\n","      <td>0.006489</td>\n","      <td>0.001634</td>\n","      <td>0.004229</td>\n","      <td>0.011743</td>\n","      <td>0.002961</td>\n","      <td>0.004978</td>\n","      <td>0.005232</td>\n","      <td>0.020566</td>\n","      <td>0.002514</td>\n","      <td>0.000875</td>\n","      <td>0.000544</td>\n","      <td>0.000117</td>\n","      <td>0.017614</td>\n","      <td>0.008286</td>\n","      <td>0.008071</td>\n","      <td>0.002916</td>\n","      <td>0.006970</td>\n","      <td>0.003734</td>\n","      <td>0.000153</td>\n","      <td>0.006404</td>\n","      <td>0.001455</td>\n","      <td>0.004553</td>\n","      <td>0.000015</td>\n","      <td>0.002443</td>\n","      <td>1.723108e-04</td>\n","      <td>0.003667</td>\n","      <td>0.018797</td>\n","      <td>0.004567</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.036183</td>\n","      <td>0.140838</td>\n","      <td>0.011000</td>\n","      <td>0.033180</td>\n","      <td>0.003269</td>\n","      <td>0.031140</td>\n","      <td>0.089108</td>\n","      <td>0.001850</td>\n","      <td>0.010664</td>\n","      <td>0.014773</td>\n","      <td>0.073684</td>\n","      <td>0.062167</td>\n","      <td>0.019646</td>\n","      <td>0.028769</td>\n","      <td>0.008238</td>\n","      <td>0.013258</td>\n","      <td>0.065749</td>\n","      <td>0.000162</td>\n","      <td>0.003680</td>\n","      <td>0.005230</td>\n","      <td>0.004396</td>\n","      <td>0.007188</td>\n","      <td>0.002522</td>\n","      <td>0.007046</td>\n","      <td>0.005208</td>\n","      <td>0.001247</td>\n","      <td>0.000441</td>\n","      <td>0.004965</td>\n","      <td>0.001395</td>\n","      <td>0.009279</td>\n","      <td>0.002631</td>\n","      <td>0.003987</td>\n","      <td>0.009714</td>\n","      <td>0.001931</td>\n","      <td>0.006673</td>\n","      <td>0.006681</td>\n","      <td>0.001205</td>\n","      <td>0.001968</td>\n","      <td>0.001459</td>\n","      <td>0.009433</td>\n","      <td>...</td>\n","      <td>0.000579</td>\n","      <td>0.000428</td>\n","      <td>0.001773</td>\n","      <td>0.008101</td>\n","      <td>0.000063</td>\n","      <td>0.000085</td>\n","      <td>0.000730</td>\n","      <td>0.000253</td>\n","      <td>0.000881</td>\n","      <td>0.000338</td>\n","      <td>0.000268</td>\n","      <td>6.105445e-07</td>\n","      <td>0.003800</td>\n","      <td>0.001943</td>\n","      <td>0.001480</td>\n","      <td>0.001093</td>\n","      <td>0.001291</td>\n","      <td>0.000648</td>\n","      <td>0.004992</td>\n","      <td>0.000512</td>\n","      <td>0.000386</td>\n","      <td>0.000109</td>\n","      <td>0.000492</td>\n","      <td>0.000628</td>\n","      <td>0.000186</td>\n","      <td>0.000144</td>\n","      <td>0.001050</td>\n","      <td>0.000188</td>\n","      <td>0.002643</td>\n","      <td>0.007928</td>\n","      <td>0.000620</td>\n","      <td>0.001317</td>\n","      <td>0.002479</td>\n","      <td>0.000834</td>\n","      <td>0.000006</td>\n","      <td>0.000744</td>\n","      <td>1.384791e-05</td>\n","      <td>0.002950</td>\n","      <td>0.006796</td>\n","      <td>0.000557</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>245</th>\n","      <td>0.822464</td>\n","      <td>0.116953</td>\n","      <td>0.099455</td>\n","      <td>0.231498</td>\n","      <td>0.014819</td>\n","      <td>0.011812</td>\n","      <td>0.024484</td>\n","      <td>0.003191</td>\n","      <td>0.055889</td>\n","      <td>0.005527</td>\n","      <td>0.028139</td>\n","      <td>0.021848</td>\n","      <td>0.050238</td>\n","      <td>0.029250</td>\n","      <td>0.048542</td>\n","      <td>0.007485</td>\n","      <td>0.027177</td>\n","      <td>0.000817</td>\n","      <td>0.006023</td>\n","      <td>0.027513</td>\n","      <td>0.004378</td>\n","      <td>0.059714</td>\n","      <td>0.008574</td>\n","      <td>0.015589</td>\n","      <td>0.008950</td>\n","      <td>0.001354</td>\n","      <td>0.001068</td>\n","      <td>0.013025</td>\n","      <td>0.011350</td>\n","      <td>0.010975</td>\n","      <td>0.127478</td>\n","      <td>0.001605</td>\n","      <td>0.003126</td>\n","      <td>0.020276</td>\n","      <td>0.001886</td>\n","      <td>0.004179</td>\n","      <td>0.008391</td>\n","      <td>0.020847</td>\n","      <td>0.006098</td>\n","      <td>0.046937</td>\n","      <td>...</td>\n","      <td>0.000670</td>\n","      <td>0.013172</td>\n","      <td>0.025200</td>\n","      <td>0.002330</td>\n","      <td>0.001164</td>\n","      <td>0.000635</td>\n","      <td>0.001818</td>\n","      <td>0.001627</td>\n","      <td>0.010293</td>\n","      <td>0.007853</td>\n","      <td>0.004076</td>\n","      <td>2.870429e-05</td>\n","      <td>0.012164</td>\n","      <td>0.000412</td>\n","      <td>0.000897</td>\n","      <td>0.000432</td>\n","      <td>0.001271</td>\n","      <td>0.003721</td>\n","      <td>0.020734</td>\n","      <td>0.004233</td>\n","      <td>0.000863</td>\n","      <td>0.002406</td>\n","      <td>0.000457</td>\n","      <td>0.002678</td>\n","      <td>0.000282</td>\n","      <td>0.000673</td>\n","      <td>0.000317</td>\n","      <td>0.001052</td>\n","      <td>0.008120</td>\n","      <td>0.002217</td>\n","      <td>0.002029</td>\n","      <td>0.005690</td>\n","      <td>0.001787</td>\n","      <td>0.002527</td>\n","      <td>0.000107</td>\n","      <td>0.016846</td>\n","      <td>1.463807e-05</td>\n","      <td>0.029586</td>\n","      <td>0.040989</td>\n","      <td>0.003993</td>\n","    </tr>\n","    <tr>\n","      <th>246</th>\n","      <td>0.299520</td>\n","      <td>0.012187</td>\n","      <td>0.059379</td>\n","      <td>0.008447</td>\n","      <td>0.003909</td>\n","      <td>0.022701</td>\n","      <td>0.012725</td>\n","      <td>0.003079</td>\n","      <td>0.025015</td>\n","      <td>0.001467</td>\n","      <td>0.100859</td>\n","      <td>0.011793</td>\n","      <td>0.005961</td>\n","      <td>0.011123</td>\n","      <td>0.008847</td>\n","      <td>0.006501</td>\n","      <td>0.006756</td>\n","      <td>0.003014</td>\n","      <td>0.014665</td>\n","      <td>0.007532</td>\n","      <td>0.001822</td>\n","      <td>0.007299</td>\n","      <td>0.000719</td>\n","      <td>0.079493</td>\n","      <td>0.008720</td>\n","      <td>0.006261</td>\n","      <td>0.000150</td>\n","      <td>0.001747</td>\n","      <td>0.004017</td>\n","      <td>0.003680</td>\n","      <td>0.004796</td>\n","      <td>0.009907</td>\n","      <td>0.001510</td>\n","      <td>0.003016</td>\n","      <td>0.031806</td>\n","      <td>0.009772</td>\n","      <td>0.000406</td>\n","      <td>0.000326</td>\n","      <td>0.000722</td>\n","      <td>0.004697</td>\n","      <td>...</td>\n","      <td>0.000377</td>\n","      <td>0.000585</td>\n","      <td>0.005768</td>\n","      <td>0.004704</td>\n","      <td>0.000374</td>\n","      <td>0.000017</td>\n","      <td>0.000292</td>\n","      <td>0.001191</td>\n","      <td>0.006545</td>\n","      <td>0.001467</td>\n","      <td>0.000219</td>\n","      <td>6.530270e-07</td>\n","      <td>0.001159</td>\n","      <td>0.002259</td>\n","      <td>0.000876</td>\n","      <td>0.000276</td>\n","      <td>0.001237</td>\n","      <td>0.000570</td>\n","      <td>0.003165</td>\n","      <td>0.000171</td>\n","      <td>0.002673</td>\n","      <td>0.000274</td>\n","      <td>0.000603</td>\n","      <td>0.001238</td>\n","      <td>0.000034</td>\n","      <td>0.000187</td>\n","      <td>0.001087</td>\n","      <td>0.000180</td>\n","      <td>0.001169</td>\n","      <td>0.007119</td>\n","      <td>0.002404</td>\n","      <td>0.000858</td>\n","      <td>0.001130</td>\n","      <td>0.000621</td>\n","      <td>0.000007</td>\n","      <td>0.007589</td>\n","      <td>8.969509e-06</td>\n","      <td>0.002372</td>\n","      <td>0.011106</td>\n","      <td>0.000151</td>\n","    </tr>\n","    <tr>\n","      <th>247</th>\n","      <td>0.025860</td>\n","      <td>0.035300</td>\n","      <td>0.044993</td>\n","      <td>0.024750</td>\n","      <td>0.008287</td>\n","      <td>0.024105</td>\n","      <td>0.069407</td>\n","      <td>0.002699</td>\n","      <td>0.068661</td>\n","      <td>0.007802</td>\n","      <td>0.048658</td>\n","      <td>0.011297</td>\n","      <td>0.004645</td>\n","      <td>0.005382</td>\n","      <td>0.004761</td>\n","      <td>0.039343</td>\n","      <td>0.006941</td>\n","      <td>0.000475</td>\n","      <td>0.010528</td>\n","      <td>0.002005</td>\n","      <td>0.009855</td>\n","      <td>0.000913</td>\n","      <td>0.006717</td>\n","      <td>0.002826</td>\n","      <td>0.006940</td>\n","      <td>0.012972</td>\n","      <td>0.005755</td>\n","      <td>0.014671</td>\n","      <td>0.006189</td>\n","      <td>0.002943</td>\n","      <td>0.004030</td>\n","      <td>0.009258</td>\n","      <td>0.002860</td>\n","      <td>0.000189</td>\n","      <td>0.006969</td>\n","      <td>0.003741</td>\n","      <td>0.001790</td>\n","      <td>0.008027</td>\n","      <td>0.001115</td>\n","      <td>0.007753</td>\n","      <td>...</td>\n","      <td>0.001430</td>\n","      <td>0.000393</td>\n","      <td>0.001592</td>\n","      <td>0.004147</td>\n","      <td>0.000314</td>\n","      <td>0.000102</td>\n","      <td>0.001193</td>\n","      <td>0.002473</td>\n","      <td>0.000611</td>\n","      <td>0.000313</td>\n","      <td>0.000278</td>\n","      <td>2.762431e-06</td>\n","      <td>0.000885</td>\n","      <td>0.003092</td>\n","      <td>0.000660</td>\n","      <td>0.001390</td>\n","      <td>0.002535</td>\n","      <td>0.000206</td>\n","      <td>0.019458</td>\n","      <td>0.001219</td>\n","      <td>0.002750</td>\n","      <td>0.000372</td>\n","      <td>0.000157</td>\n","      <td>0.000807</td>\n","      <td>0.000909</td>\n","      <td>0.000106</td>\n","      <td>0.002089</td>\n","      <td>0.001247</td>\n","      <td>0.002491</td>\n","      <td>0.002344</td>\n","      <td>0.000760</td>\n","      <td>0.004509</td>\n","      <td>0.001457</td>\n","      <td>0.000368</td>\n","      <td>0.000027</td>\n","      <td>0.008311</td>\n","      <td>4.010853e-05</td>\n","      <td>0.005471</td>\n","      <td>0.005690</td>\n","      <td>0.000623</td>\n","    </tr>\n","    <tr>\n","      <th>248</th>\n","      <td>0.637358</td>\n","      <td>0.064582</td>\n","      <td>0.021150</td>\n","      <td>0.016291</td>\n","      <td>0.016730</td>\n","      <td>0.012684</td>\n","      <td>0.045350</td>\n","      <td>0.000909</td>\n","      <td>0.058214</td>\n","      <td>0.001165</td>\n","      <td>0.014379</td>\n","      <td>0.024243</td>\n","      <td>0.006073</td>\n","      <td>0.014412</td>\n","      <td>0.041080</td>\n","      <td>0.007941</td>\n","      <td>0.004208</td>\n","      <td>0.000889</td>\n","      <td>0.006333</td>\n","      <td>0.001511</td>\n","      <td>0.020864</td>\n","      <td>0.013007</td>\n","      <td>0.002262</td>\n","      <td>0.007635</td>\n","      <td>0.007554</td>\n","      <td>0.009547</td>\n","      <td>0.000224</td>\n","      <td>0.004155</td>\n","      <td>0.006646</td>\n","      <td>0.004506</td>\n","      <td>0.034617</td>\n","      <td>0.003612</td>\n","      <td>0.001139</td>\n","      <td>0.000388</td>\n","      <td>0.000937</td>\n","      <td>0.008660</td>\n","      <td>0.003197</td>\n","      <td>0.001678</td>\n","      <td>0.004041</td>\n","      <td>0.004180</td>\n","      <td>...</td>\n","      <td>0.000633</td>\n","      <td>0.000259</td>\n","      <td>0.005810</td>\n","      <td>0.000776</td>\n","      <td>0.000325</td>\n","      <td>0.000033</td>\n","      <td>0.000522</td>\n","      <td>0.002927</td>\n","      <td>0.006486</td>\n","      <td>0.000429</td>\n","      <td>0.000098</td>\n","      <td>9.617871e-06</td>\n","      <td>0.003228</td>\n","      <td>0.000701</td>\n","      <td>0.000359</td>\n","      <td>0.000148</td>\n","      <td>0.000921</td>\n","      <td>0.000396</td>\n","      <td>0.005910</td>\n","      <td>0.000749</td>\n","      <td>0.001413</td>\n","      <td>0.000725</td>\n","      <td>0.000301</td>\n","      <td>0.001279</td>\n","      <td>0.000110</td>\n","      <td>0.000177</td>\n","      <td>0.000427</td>\n","      <td>0.000164</td>\n","      <td>0.001381</td>\n","      <td>0.003933</td>\n","      <td>0.005158</td>\n","      <td>0.003377</td>\n","      <td>0.002263</td>\n","      <td>0.000927</td>\n","      <td>0.000046</td>\n","      <td>0.008061</td>\n","      <td>9.849225e-07</td>\n","      <td>0.005937</td>\n","      <td>0.002481</td>\n","      <td>0.001134</td>\n","    </tr>\n","    <tr>\n","      <th>249</th>\n","      <td>0.254831</td>\n","      <td>0.064011</td>\n","      <td>0.466634</td>\n","      <td>0.679612</td>\n","      <td>0.027657</td>\n","      <td>0.090727</td>\n","      <td>0.007031</td>\n","      <td>0.009860</td>\n","      <td>0.017780</td>\n","      <td>0.046358</td>\n","      <td>0.082542</td>\n","      <td>0.065181</td>\n","      <td>0.010136</td>\n","      <td>0.114760</td>\n","      <td>0.040054</td>\n","      <td>0.005123</td>\n","      <td>0.016401</td>\n","      <td>0.000166</td>\n","      <td>0.007396</td>\n","      <td>0.024299</td>\n","      <td>0.001544</td>\n","      <td>0.032905</td>\n","      <td>0.003453</td>\n","      <td>0.019323</td>\n","      <td>0.003831</td>\n","      <td>0.001291</td>\n","      <td>0.000759</td>\n","      <td>0.033467</td>\n","      <td>0.005214</td>\n","      <td>0.005421</td>\n","      <td>0.169088</td>\n","      <td>0.000754</td>\n","      <td>0.002845</td>\n","      <td>0.074486</td>\n","      <td>0.000628</td>\n","      <td>0.003905</td>\n","      <td>0.018497</td>\n","      <td>0.020204</td>\n","      <td>0.010505</td>\n","      <td>0.017227</td>\n","      <td>...</td>\n","      <td>0.000505</td>\n","      <td>0.002463</td>\n","      <td>0.005591</td>\n","      <td>0.003843</td>\n","      <td>0.000247</td>\n","      <td>0.000684</td>\n","      <td>0.007163</td>\n","      <td>0.005124</td>\n","      <td>0.009344</td>\n","      <td>0.001632</td>\n","      <td>0.002228</td>\n","      <td>9.804114e-05</td>\n","      <td>0.020282</td>\n","      <td>0.005000</td>\n","      <td>0.000751</td>\n","      <td>0.001148</td>\n","      <td>0.000291</td>\n","      <td>0.002112</td>\n","      <td>0.023227</td>\n","      <td>0.003246</td>\n","      <td>0.000287</td>\n","      <td>0.001336</td>\n","      <td>0.000718</td>\n","      <td>0.065514</td>\n","      <td>0.000083</td>\n","      <td>0.000496</td>\n","      <td>0.000476</td>\n","      <td>0.002167</td>\n","      <td>0.004163</td>\n","      <td>0.012269</td>\n","      <td>0.001721</td>\n","      <td>0.009597</td>\n","      <td>0.001819</td>\n","      <td>0.001681</td>\n","      <td>0.000032</td>\n","      <td>0.002769</td>\n","      <td>1.712468e-05</td>\n","      <td>0.022688</td>\n","      <td>0.017451</td>\n","      <td>0.001222</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>250 rows  161 columns</p>\n","</div>"],"text/plain":["          r52       r69     r50.9  ...     i50.9    r31.29    r68.89\n","0    0.044488  0.525451  0.053205  ...  0.008127  0.023733  0.039619\n","1    0.221252  0.333275  0.045961  ...  0.000972  0.006062  0.006854\n","2    0.102241  0.084894  0.105460  ...  0.125863  0.070329  0.000314\n","3    0.104872  0.394633  0.043287  ...  0.003667  0.018797  0.004567\n","4    0.036183  0.140838  0.011000  ...  0.002950  0.006796  0.000557\n","..        ...       ...       ...  ...       ...       ...       ...\n","245  0.822464  0.116953  0.099455  ...  0.029586  0.040989  0.003993\n","246  0.299520  0.012187  0.059379  ...  0.002372  0.011106  0.000151\n","247  0.025860  0.035300  0.044993  ...  0.005471  0.005690  0.000623\n","248  0.637358  0.064582  0.021150  ...  0.005937  0.002481  0.001134\n","249  0.254831  0.064011  0.466634  ...  0.022688  0.017451  0.001222\n","\n","[250 rows x 161 columns]"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"YrUWcBYwc5vp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629218303915,"user_tz":-120,"elapsed":23,"user":{"displayName":"Ignacio Sisamon Serrano","photoUrl":"","userId":"07246385471958455157"}},"outputId":"36c99f41-45b6-4061-aae7-450f1c48fe57"},"source":["outputs = submiss_df>0.5\n","f1_score_micro = metrics.f1_score(test_matrix.iloc[:,1:], outputs, average='micro')\n","f1_score_macro = metrics.f1_score(test_matrix.iloc[:,1:], outputs, average='macro')\n","ap = metrics.average_precision_score(test_matrix.iloc[:,1:], outputs, average='micro')\n","print(f1_score_micro)\n","print(f1_score_macro)\n","print(ap)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.19337748344370861\n","0.04392388032164332\n","0.12637801062606346\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning:\n","\n","F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","\n"],"name":"stderr"}]}]}